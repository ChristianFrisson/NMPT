<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Nick&#39;s Machine Perception Toolbox: InternalMotionModel Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.2 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul class="tablist">
      <li><a href="main.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
</div>
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a> &#124;
<a href="#friends">Friends</a>  </div>
  <div class="headertitle">
<h1>InternalMotionModel Class Reference<br/>
<small>
[<a class="el" href="group___m_p_group.html">Machine Perception Primitives</a>]</small>
</h1>  </div>
</div>
<div class="contents">
<!-- doxytag: class="InternalMotionModel" -->
<p><code> <b> Machine Perception Primitive: </b> </code> An implementation of a model that allows a robot to learn about its own motion.  
<a href="#_details">More...</a></p>

<p><code>#include &lt;InternalMotionModel.h&gt;</code></p>
<!-- startSectionHeader --><div class="dynheader">
Collaboration diagram for InternalMotionModel:<!-- endSectionHeader --></div>
<!-- startSectionSummary --><!-- endSectionSummary --><!-- startSectionContent --><div class="dyncontent">
<div class="center"><img src="class_internal_motion_model__coll__graph.png" border="0" usemap="#_internal_motion_model_coll__map" alt="Collaboration graph"/></div>
<map name="_internal_motion_model_coll__map" id="_internal_motion_model_coll__map">
<area shape="rect" id="node2" href="class_matrix_kalman_filter.html" title="Auxilliary Tool: A Kalman Filter for estimating the values of a matrix, which is like doing online li..." alt="" coords="5,5,144,35"/><area shape="rect" id="node4" href="class_image_kalman_filter.html" title="Auxilliary Tool: A Collection of Kalman Filters for estimating the appearance of an image..." alt="" coords="168,5,309,35"/></map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center><!-- endSectionContent --></div>

<p><a href="class_internal_motion_model-members.html">List of all members.</a></p>
<table class="memberdecls">
<tr><td colspan="2"><h2><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a69dfa852a85e6da2f1fc6af21990ae31"></a><!-- doxytag: member="InternalMotionModel::operator=" ref="a69dfa852a85e6da2f1fc6af21990ae31" args="(const InternalMotionModel &amp;rhs)" -->
<a class="el" href="class_internal_motion_model.html">InternalMotionModel</a> &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a69dfa852a85e6da2f1fc6af21990ae31">operator=</a> (const <a class="el" href="class_internal_motion_model.html">InternalMotionModel</a> &amp;rhs)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Assignment Operator. Perform a deep copy of another <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a>. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac0b90e5f309524b49c14721e77a532b1"></a><!-- doxytag: member="InternalMotionModel::InternalMotionModel" ref="ac0b90e5f309524b49c14721e77a532b1" args="(const InternalMotionModel &amp;copy)" -->
&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ac0b90e5f309524b49c14721e77a532b1">InternalMotionModel</a> (const <a class="el" href="class_internal_motion_model.html">InternalMotionModel</a> &amp;copy)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Copy Constructor. Perform a deep copy of another <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a>. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a23fd9fce8c82df2e0d73d1a9fc68972c"></a><!-- doxytag: member="InternalMotionModel::InternalMotionModel" ref="a23fd9fce8c82df2e0d73d1a9fc68972c" args="()" -->
&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a23fd9fce8c82df2e0d73d1a9fc68972c">InternalMotionModel</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Default Constructor. Equivalent to calling the main constructor with 2 servos, an observation size of (320,240), and 30 dynamics models. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ae566dde5d4180385b11edcfdc5403ed0">InternalMotionModel</a> (int numActuators, cv::Size obsSize=cv::Size(320, 240), size_t numDynamicsModels=30)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Main Constructor.  <a href="#ae566dde5d4180385b11edcfdc5403ed0"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a73f4f47c62f0aebdb7608b0f10d2e2d9"></a><!-- doxytag: member="InternalMotionModel::~InternalMotionModel" ref="a73f4f47c62f0aebdb7608b0f10d2e2d9" args="()" -->
virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a73f4f47c62f0aebdb7608b0f10d2e2d9">~InternalMotionModel</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Destructor. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="af5f60974a4589bb2c4aac1f0409d61e5"></a><!-- doxytag: member="InternalMotionModel::setNumActuators" ref="af5f60974a4589bb2c4aac1f0409d61e5" args="(int num)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#af5f60974a4589bb2c4aac1f0409d61e5">setNumActuators</a> (int num)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Resets the number of actuators that we are learning a motion model over. This resets the model to the prior model. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a1b018f118c1b28f768222544759bbb1f"></a><!-- doxytag: member="InternalMotionModel::setObsSize" ref="a1b018f118c1b28f768222544759bbb1f" args="(cv::Size s)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a1b018f118c1b28f768222544759bbb1f">setObsSize</a> (cv::Size s)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Tell the <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a> what size observations to expect. This resets any knowledge of the appearance of the world. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac8637125432963ef5cc6945665d269a4"></a><!-- doxytag: member="InternalMotionModel::setWorldSizePadFactor" ref="ac8637125432963ef5cc6945665d269a4" args="(double val=7.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ac8637125432963ef5cc6945665d269a4">setWorldSizePadFactor</a> (double val=7.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the size of the world appearance estimate in terms of the width and height of the observations. This * resets any knowledge of the appearance of the world. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a5b65b30ef8a49b820acabed7a6070577">setNumDynamicsModels</a> (size_t num)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the number of time points used for estimating the Dynamics Model. This resets the estimate of the motion parameters.  <a href="#a5b65b30ef8a49b820acabed7a6070577"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac8fcc41982e482e232912723cd9ff1b6"></a><!-- doxytag: member="InternalMotionModel::getNumDynamicsModels" ref="ac8fcc41982e482e232912723cd9ff1b6" args="() const " -->
size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ac8fcc41982e482e232912723cd9ff1b6">getNumDynamicsModels</a> () const </td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the number of time points used for estimating the Dynamics Model. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ab001a2562119eb021952df35b0c0f88f">setUseRetinalCoordinates</a> (int yesorno=1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Sets the coordinate frame to World Coordinates (0) or Retinal Coordinates (Non-0). By default, retinal coordinates are used.  <a href="#ab001a2562119eb021952df35b0c0f88f"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aad1f6f4af126e5715f23faa72762946b"></a><!-- doxytag: member="InternalMotionModel::setUseMotorUncertaintyInLearning" ref="aad1f6f4af126e5715f23faa72762946b" args="(int yesorno=0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#aad1f6f4af126e5715f23faa72762946b">setUseMotorUncertaintyInLearning</a> (int yesorno=0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">EXPERIMENTAL: Use computed motor reliabilities when updating the motion model using <a class="el" href="class_internal_motion_model.html#a87183b306fb58600c50ec9481b4ee6d2" title="Assert that tau is the correct transform (x,y translation) induced by the last taken eyemovement (act...">updateModel()</a> or <a class="el" href="class_internal_motion_model.html#a82524c549daab8c92569cf5f78709d23" title="This is the main training function. It finds the most likely offset of the seenImage in the represent...">updateModelMAP()</a>. You probably should not use this. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a961ff65fb04221da698f17613244841a"></a><!-- doxytag: member="InternalMotionModel::setUseSensorUncertaintyInLearning" ref="a961ff65fb04221da698f17613244841a" args="(int yesorno=0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a961ff65fb04221da698f17613244841a">setUseSensorUncertaintyInLearning</a> (int yesorno=0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">EXPERIMENTAL: Use computed sensor reliabilities when updating the visual model using <a class="el" href="class_internal_motion_model.html#a87183b306fb58600c50ec9481b4ee6d2" title="Assert that tau is the correct transform (x,y translation) induced by the last taken eyemovement (act...">updateModel()</a> or <a class="el" href="class_internal_motion_model.html#a82524c549daab8c92569cf5f78709d23" title="This is the main training function. It finds the most likely offset of the seenImage in the represent...">updateModelMAP()</a>. You probably should not use this. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a7386ed570844d39596ae4eb2d439ced9">setUseLearnedInverseModel</a> (int yesorno=0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">EXPERIMENTAL: Use a directly learned inverse model for computing <a class="el" href="class_internal_motion_model.html#a0ed0f0563e35ecd87eb1d56bcea5b0c0" title="Make a decision about how to look at something. This command is sufficient for low-speed tracking...">recommendActionForDesiredTransform()</a>. You probably should not use this.  <a href="#a7386ed570844d39596ae4eb2d439ced9"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a28e49fde234e38a0cf1b9eda90c8d3c4">setMuLambdaPrior</a> (double val=0.5)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the mean of Lambda intensity estimates to all be a specified value. Lambda is the estimate of the world appearance.  <a href="#a28e49fde234e38a0cf1b9eda90c8d3c4"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a5ed87ca576afece3bbd17fde5b136f4c">setSigmaSquaredLambdaPrior</a> (double val=0.25)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the variance of the Image intensity estimates to all be a specified value. Lambda is the estimate of the world appearance.  <a href="#a5ed87ca576afece3bbd17fde5b136f4c"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#abfa0f7139626d149e371b62c7393b4b4">setRSquaredLambda</a> (double std=0.0025)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the pixel drift variance (how much do pixels change their value over time?). Lambda is the estimate of the world appearance.  <a href="#abfa0f7139626d149e371b62c7393b4b4"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ac1c7eacd0995e054d2ca17c96f12a30c">setQSquaredLambda</a> (double std=0.01)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the camera noise variance (how much do you trust an unreliable sensor?). Lambda is the estimate of the world appearance.  <a href="#ac1c7eacd0995e054d2ca17c96f12a30c"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a695fc8035de43f47c1b8c22f91543480">setMuEtaPrior</a> (double val=0.5)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the mean of Lambda intensity estimates to all be a specified value. Eta is an estimate of which areas of the world change quickly or slowly (have high motion).  <a href="#a695fc8035de43f47c1b8c22f91543480"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ad73e20fe0f426d5872e1b1eccd1f9642">setSigmaSquaredEtaPrior</a> (double val=0.25)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the variance of the Image intensity estimates to all be a specified value. Eta is an estimate of which areas of the world change quickly or slowly (have high motion).  <a href="#ad73e20fe0f426d5872e1b1eccd1f9642"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#aa44f0646d9b3b5093fc0056257b22d93">setRSquaredEta</a> (double std=0.0001)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the pixel drift variance (how much do pixels change their value over time?). Eta is an estimate of which areas of the world change quickly or slowly (have high motion).  <a href="#aa44f0646d9b3b5093fc0056257b22d93"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ab48e6a8352d7e5f06e6b3b7e64b7e1f7">setQSquaredEta</a> (double std=0.01)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the camera noise variance (how much do you trust an unreliable sensor?). Eta is an estimate of which areas of the world change quickly or slowly (have high motion).  <a href="#ab48e6a8352d7e5f06e6b3b7e64b7e1f7"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a3bda310ebeba416d8aad134c7f91c571"></a><!-- doxytag: member="InternalMotionModel::setMuAlphaPrior" ref="a3bda310ebeba416d8aad134c7f91c571" args="(double vectorVal=0.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a3bda310ebeba416d8aad134c7f91c571">setMuAlphaPrior</a> (double vectorVal=0.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the mean of the motion model parameters Alpha to have each element with identical value. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a2455430768afb25c0c4c9f3ddcde028e"></a><!-- doxytag: member="InternalMotionModel::setSigmaAlphaPrior" ref="a2455430768afb25c0c4c9f3ddcde028e" args="(double diagval=250000.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a2455430768afb25c0c4c9f3ddcde028e">setSigmaAlphaPrior</a> (double diagval=250000.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the uncertainty in the motion model parameters Alpha to a diagonal matrix with identical values along the diagonal. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="abf3f896fb1f39b4e0fcc831adf7fd8e5"></a><!-- doxytag: member="InternalMotionModel::setRAlpha" ref="abf3f896fb1f39b4e0fcc831adf7fd8e5" args="(double diagval=25.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#abf3f896fb1f39b4e0fcc831adf7fd8e5">setRAlpha</a> (double diagval=25.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set R, the drift in the estimate of the motion model parameters Alpha to a diagonal matrix with identical values along the diagonal. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a83e466ff9b9a0e2f2d990c555a378501"></a><!-- doxytag: member="InternalMotionModel::setQAlpha" ref="a83e466ff9b9a0e2f2d990c555a378501" args="(double diagval=400.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a83e466ff9b9a0e2f2d990c555a378501">setQAlpha</a> (double diagval=400.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set Q, the noise in observations of the transforms tau (x,y translation) to a diagonal matrix with identical values along the diagonal. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a0c2c198ae3ad0b53ca63cc5a2f76ba33"></a><!-- doxytag: member="InternalMotionModel::setMuNuPrior" ref="a0c2c198ae3ad0b53ca63cc5a2f76ba33" args="(double vectorVal=20)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a0c2c198ae3ad0b53ca63cc5a2f76ba33">setMuNuPrior</a> (double vectorVal=20)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the mean of the motor noise model parameters Nu to have each element with identical value. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a004749c87fd3ebeeae1d01bc9f8a2267"></a><!-- doxytag: member="InternalMotionModel::setSigmaNuPrior" ref="a004749c87fd3ebeeae1d01bc9f8a2267" args="(double diagval=250000.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a004749c87fd3ebeeae1d01bc9f8a2267">setSigmaNuPrior</a> (double diagval=250000.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set the uncertainty in the motor noise model parameters Nu to a diagonal matrix with identical values along the diagonal. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad6e3ad3d2ef831671991fe67c7114780"></a><!-- doxytag: member="InternalMotionModel::setRNu" ref="ad6e3ad3d2ef831671991fe67c7114780" args="(double diagval=25.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ad6e3ad3d2ef831671991fe67c7114780">setRNu</a> (double diagval=25.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set R, the drift in the estimate of the motor noise model parameters Nu, to a diagonal matrix with identical values along the diagonal. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9a58a07a7f99030251f10c4210d65cfa"></a><!-- doxytag: member="InternalMotionModel::setQNu" ref="a9a58a07a7f99030251f10c4210d65cfa" args="(double diagval=400.0)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a9a58a07a7f99030251f10c4210d65cfa">setQNu</a> (double diagval=400.0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Set Q, the error in the estimate of the transforms tau (x,y translation), to a diagonal matrix with identical values along the diagonal. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a0ed0f0563e35ecd87eb1d56bcea5b0c0">recommendActionForDesiredTransform</a> (const cv::Mat &amp;tau, cv::Mat &amp;action)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Make a decision about how to look at something. This command is sufficient for low-speed tracking, in which each decision about where to move next is made after the previous eye-movement has completed, on the order of 1s. For* high-speed trackinging, where decsions are made faster than about 300ms, it is important to use the other recommendActionForDesiredTransform which takes an actionHistory and decisionTimeStamp as arguments.  <a href="#a0ed0f0563e35ecd87eb1d56bcea5b0c0"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a37e31148bf8dda83731afc557cd3ad9b">recommendActionForDesiredTransform</a> (const cv::Mat &amp;tau, const std::vector&lt; <a class="el" href="structaction_record.html">actionRecord</a> &gt; &amp;history, double decisionTimeStamp, cv::Mat &amp;action)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Make a decision about how to look at something, taking the dynamics model and a history of eye-movements into account. Only use this command if you have trained the InternalMotionModel's dynamics model, and are making decisions about where to move the eyes on a sub-second timescale.  <a href="#a37e31148bf8dda83731afc557cd3ad9b"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">cv::Size&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a4ba969effe1049866359793f33366a9f">getEyeMovementUnitsSize</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Get the units that recommendActionForDesiredTransform is expecting.  <a href="#a4ba969effe1049866359793f33366a9f"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a82524c549daab8c92569cf5f78709d23">updateModelMAP</a> (IplImage *seenImage, const cv::Mat &amp;action, int frameNum=-1, double frameTime=0)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">This is the main training function. It finds the most likely offset of the seenImage in the representation of remembered visual history, and uses that, along with the action taken, to train the Motion Model. Optionally, frameNum and frameTime can be supplied to train the DynamicsModel.  <a href="#a82524c549daab8c92569cf5f78709d23"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a9b038b0a33e5163674820c4cc356cae7"></a><!-- doxytag: member="InternalMotionModel::getTauMeanForAction" ref="a9b038b0a33e5163674820c4cc356cae7" args="(const cv::Mat &amp;action, cv::Mat &amp;tauMat, int frameNum=&#45;1)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a9b038b0a33e5163674820c4cc356cae7">getTauMeanForAction</a> (const cv::Mat &amp;action, cv::Mat &amp;tauMat, int frameNum=-1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Find the mean transform (x,y translation) predicted by the current motion model for a given action. The result is passed back through the tauMat reference. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a427ac776a860d61b6a4c7c70212ef218"></a><!-- doxytag: member="InternalMotionModel::obsLogLikelihood" ref="a427ac776a860d61b6a4c7c70212ef218" args="(const cv::Mat &amp;tau, IplImage *seenImage, const cv::Mat &amp;action, int frameNum=&#45;1)" -->
double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a427ac776a860d61b6a4c7c70212ef218">obsLogLikelihood</a> (const cv::Mat &amp;tau, IplImage *seenImage, const cv::Mat &amp;action, int frameNum=-1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Find the log likelihood of a particular transform tau (x,y translation) give an seenImage from the camera and action, the last taken eye-movement. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a486fab952663ef8ebd415520d09b815f"></a><!-- doxytag: member="InternalMotionModel::findMaxLikelihoodTransform" ref="a486fab952663ef8ebd415520d09b815f" args="(IplImage *seenImage, const cv::Mat &amp;action, cv::Mat &amp;tau, int frameNum=&#45;1)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a486fab952663ef8ebd415520d09b815f">findMaxLikelihoodTransform</a> (IplImage *seenImage, const cv::Mat &amp;action, cv::Mat &amp;tau, int frameNum=-1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Find the transform (x,y translation) that maximizes obsLogLikelihood. The result is passed back through the reference to the matrix tau. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a3f3fa4091055991ef2edd4b3ce2c544d"></a><!-- doxytag: member="InternalMotionModel::findMaxLikelihoodTransformFaster" ref="a3f3fa4091055991ef2edd4b3ce2c544d" args="(IplImage *seenImage, const cv::Mat &amp;action, cv::Mat &amp;tau, int frameNum=&#45;1)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a3f3fa4091055991ef2edd4b3ce2c544d">findMaxLikelihoodTransformFaster</a> (IplImage *seenImage, const cv::Mat &amp;action, cv::Mat &amp;tau, int frameNum=-1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Find the transform (x,y translation) that approximately maximizes obsLogLikelihood. The result is passed back through the reference to the matrix tau. This function is much faster (at a negligible cost to accuracy) than <a class="el" href="class_internal_motion_model.html#a486fab952663ef8ebd415520d09b815f" title="Find the transform (x,y translation) that maximizes obsLogLikelihood. The result is passed back throu...">findMaxLikelihoodTransform()</a>. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a87183b306fb58600c50ec9481b4ee6d2"></a><!-- doxytag: member="InternalMotionModel::updateModel" ref="a87183b306fb58600c50ec9481b4ee6d2" args="(IplImage *seenImage, const cv::Mat &amp;action, const cv::Mat &amp;tau, int frameNum=&#45;1)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a87183b306fb58600c50ec9481b4ee6d2">updateModel</a> (IplImage *seenImage, const cv::Mat &amp;action, const cv::Mat &amp;tau, int frameNum=-1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Assert that tau is the correct transform (x,y translation) induced by the last taken eyemovement (action). This updates the appearance model of the world with the seenImage, and the model of the robot's motion with action. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4e47a43657ae3362cd8990015a0f3189"></a><!-- doxytag: member="InternalMotionModel::updateFrameTime" ref="a4e47a43657ae3362cd8990015a0f3189" args="(int frameNum, double frameTime, double timeConst=0.1)" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a4e47a43657ae3362cd8990015a0f3189">updateFrameTime</a> (int frameNum, double frameTime, double timeConst=0.1)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Find the transform (x,y translation) that approximately maximizes obsLogLikelihood. The result is passed back through the reference to the matrix tau. This function is much faster (at a negligible cost to accuracy) than <a class="el" href="class_internal_motion_model.html#a486fab952663ef8ebd415520d09b815f" title="Find the transform (x,y translation) that maximizes obsLogLikelihood. The result is passed back throu...">findMaxLikelihoodTransform()</a>. <br/></td></tr>
<tr><td colspan="2"><h2><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="aad124b878a048bfdbb7114a437aaa2a6"></a><!-- doxytag: member="InternalMotionModel::lambda" ref="aad124b878a048bfdbb7114a437aaa2a6" args="" -->
<a class="el" href="class_image_kalman_filter.html">ImageKalmanFilter</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#aad124b878a048bfdbb7114a437aaa2a6">lambda</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">The estimate of the appearance of the world around the robot. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad6005f75dfd91f28e5c723b12044c913"></a><!-- doxytag: member="InternalMotionModel::eta" ref="ad6005f75dfd91f28e5c723b12044c913" args="" -->
<a class="el" href="class_image_kalman_filter.html">ImageKalmanFilter</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ad6005f75dfd91f28e5c723b12044c913">eta</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">An estimate of the variability of the world around the robot, which will be high in regions where the sensors are unreliable, and objects are likely to move. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a89a21f993b9ca240eaab553477f8d927"></a><!-- doxytag: member="InternalMotionModel::alpha" ref="a89a21f993b9ca240eaab553477f8d927" args="" -->
<a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a89a21f993b9ca240eaab553477f8d927">alpha</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">An estimate of the motor gains learned by the robot. This points to the "steady state" dynamics which are used for making eye-movements on a slow timescale. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a2678917f2b8cc64130596d7f428b95c7"></a><!-- doxytag: member="InternalMotionModel::nu" ref="a2678917f2b8cc64130596d7f428b95c7" args="" -->
<a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a2678917f2b8cc64130596d7f428b95c7">nu</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">An estimate of the reliability of the different individual motors. This information is currently used for doing reliable inverse kinematics. The pointer is to the "steady state" dynamics which are used for making eye-movements on a slow timescale. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a27baa5ff19b247c4f06a752d693824ed">alphaDynamics</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">std::vector&lt; <a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a> &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ad39cc9e30223ac06b5109cfb3a78364b">nuDynamics</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">std::vector&lt; double &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a41c762614e142ce0dc2766456d2f9d1a">timeStamps</a></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a2931cedd189ee28e2ba139247a676787"></a><!-- doxytag: member="InternalMotionModel::inverseMotionModel" ref="a2931cedd189ee28e2ba139247a676787" args="" -->
<a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a>&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#a2931cedd189ee28e2ba139247a676787">inverseMotionModel</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">A directly learned inverse model that can be used by <a class="el" href="class_internal_motion_model.html#a0ed0f0563e35ecd87eb1d56bcea5b0c0" title="Make a decision about how to look at something. This command is sufficient for low-speed tracking...">recommendActionForDesiredTransform()</a>. It can be enabled using setUseLearnedInverseModel(1), but it should not be. It is unreliable when there are more motors than translation dimensions, and the computed inverse method used by default is much better at taking the reliability of the individaul motors into account. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. <br/></td></tr>
<tr><td colspan="2"><h2><a name="friends"></a>
Friends</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ad0b452c28b5dd3c2494e3c8a126f8c22"></a><!-- doxytag: member="InternalMotionModel::operator&lt;&lt;" ref="ad0b452c28b5dd3c2494e3c8a126f8c22" args="(std::ostream &amp;ofs, const InternalMotionModel &amp;feat)" -->
std::ostream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ad0b452c28b5dd3c2494e3c8a126f8c22">operator&lt;&lt;</a> (std::ostream &amp;ofs, const <a class="el" href="class_internal_motion_model.html">InternalMotionModel</a> &amp;feat)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Write to a file. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac92b3dd294179c9b98df00198d98479a"></a><!-- doxytag: member="InternalMotionModel::operator&gt;&gt;" ref="ac92b3dd294179c9b98df00198d98479a" args="(std::istream &amp;ifs, InternalMotionModel &amp;feat)" -->
std::istream &amp;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_internal_motion_model.html#ac92b3dd294179c9b98df00198d98479a">operator&gt;&gt;</a> (std::istream &amp;ifs, <a class="el" href="class_internal_motion_model.html">InternalMotionModel</a> &amp;feat)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Read from a file. <br/></td></tr>
</table>
<hr/><a name="_details"></a><h2>Detailed Description</h2>
<p><code> <b> Machine Perception Primitive: </b> </code> An implementation of a model that allows a robot to learn about its own motion. </p>
<p>This code is an extension of the approach first described in Butko and Movellan, "Learning to Look," 2010 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>).</p>
<p>The default parameters were found to work well for three different robots. Hopefully they will work for your robot too.</p>
<p>For learning, it is sufficient to call "updateModelMAP" with the robot's current field of view (a 320x240 floating point image) and previous action. After learning, appropriate motor commands can be found using "recommendActionForDesiredTransform".</p>
<p>Actions should be represented using a relative coordinate system such that 0 means "no change," and the sign indicates the direction of motion. The default parameters were chosen in a situation where an action of 1 represented the largest possible movement on a given motor (e.g. from the left-most end of the motor's range to the right-most). So, an action of 1 would always move the motor to the right-most extreme, regardless of where you started, -1 would always move the motor to the left-most extreme. A value of .1 would move the motor 1/10th of the total range of motion. Sending an action of -1, +1 would result in the same end point as calling -1, +1/3, +1/3, +1/3. This endpoint will be the same regardless of starting point because the intial -1 will always saturate the motor's range, pushing it to the extreme.</p>
<dl class="author"><dt><b>Author:</b></dt><dd>Nicholas Butko </dd></dl>
<dl class="date"><dt><b>Date:</b></dt><dd>2010 </dd></dl>
<dl class="version"><dt><b>Version:</b></dt><dd>0.4 </dd></dl>
<hr/><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="ae566dde5d4180385b11edcfdc5403ed0"></a><!-- doxytag: member="InternalMotionModel::InternalMotionModel" ref="ae566dde5d4180385b11edcfdc5403ed0" args="(int numActuators, cv::Size obsSize=cv::Size(320, 240), size_t numDynamicsModels=30)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">InternalMotionModel::InternalMotionModel </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>numActuators</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">cv::Size&#160;</td>
          <td class="paramname"> <em>obsSize</em> = <code>cv::Size(320,&#160;240)</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"> <em>numDynamicsModels</em> = <code>30</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Main Constructor. </p>
<p>The internal motion model takes three arguments: numActuators (degrees of freedom that affect camera motion), obsSize (size of the image grabbed by the camera), and numDynamicsModels.</p>
<p>The dynamics models are important for high speed tracking, but they are not necessary for the basic behavior of the system to work. The dynamics model is trained by having separate time-points at frame-rate intervals after an eye movement. For example, if the camera is 30FPS, then the learning protocal would be to issue an eye movement, collect the next 30 frames, and then call updateModelMAP(image[i], action, i) for each of the 30 images in turn.</p>
<p>Calling updateModelMAP(image, action) will only update the latest timepoint model, which is the one used by <a class="el" href="class_internal_motion_model.html#a0ed0f0563e35ecd87eb1d56bcea5b0c0" title="Make a decision about how to look at something. This command is sufficient for low-speed tracking...">recommendActionForDesiredTransform()</a> without an action history. </p>

</div>
</div>
<hr/><h2>Member Function Documentation</h2>
<a class="anchor" id="a4ba969effe1049866359793f33366a9f"></a><!-- doxytag: member="InternalMotionModel::getEyeMovementUnitsSize" ref="a4ba969effe1049866359793f33366a9f" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Size InternalMotionModel::getEyeMovementUnitsSize </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Get the units that recommendActionForDesiredTransform is expecting. </p>
<p>This returns the observation size the this <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a> was trained with, which is needed to request a "desiredTransform". The units of the desired transform are -getEyeMovementSize().width/2:getEyeMovementSize().width/2 and the analog for height. It is possible to request an eye-movement outside this range, which is simply equivalent to asking the robot to turn to look at something outside its current field of view. </p>

</div>
</div>
<a class="anchor" id="a0ed0f0563e35ecd87eb1d56bcea5b0c0"></a><!-- doxytag: member="InternalMotionModel::recommendActionForDesiredTransform" ref="a0ed0f0563e35ecd87eb1d56bcea5b0c0" args="(const cv::Mat &amp;tau, cv::Mat &amp;action)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::recommendActionForDesiredTransform </td>
          <td>(</td>
          <td class="paramtype">const cv::Mat &amp;&#160;</td>
          <td class="paramname"> <em>tau</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">cv::Mat &amp;&#160;</td>
          <td class="paramname"> <em>action</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Make a decision about how to look at something. This command is sufficient for low-speed tracking, in which each decision about where to move next is made after the previous eye-movement has completed, on the order of 1s. For* high-speed trackinging, where decsions are made faster than about 300ms, it is important to use the other recommendActionForDesiredTransform which takes an actionHistory and decisionTimeStamp as arguments. </p>
<p>The desired transform (tau) is a desired pixel translation. When using retinal coordinates, the translation 0, 0 is the central pixel of the current visual field.</p>
<p>The result is passed by reference back through the parameter action. This is the command you should send to the robot servos.</p>
<p>The units of tau are pixel-based, with respect to the original image size used for training the motion model. For example, if the model was trained with images of size 320x240, and you want to look at the top left of the visual field, then tau should be (160,-120). Note that you are responsible for making this be the case. It's common to have the runtime visual field be higher resolution than that used in training, for example 640x480. In this case, you would want to scale your pixel units by a factor of two. To know what your particular scaling factor should be, call "getEyeMovementUnitsSize()" to find the size of image that the <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a> was originally trained with, and scale your transforms accordingly before calling recommendActionForDesiredTransform. </p>

</div>
</div>
<a class="anchor" id="a37e31148bf8dda83731afc557cd3ad9b"></a><!-- doxytag: member="InternalMotionModel::recommendActionForDesiredTransform" ref="a37e31148bf8dda83731afc557cd3ad9b" args="(const cv::Mat &amp;tau, const std::vector&lt; actionRecord &gt; &amp;history, double decisionTimeStamp, cv::Mat &amp;action)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::recommendActionForDesiredTransform </td>
          <td>(</td>
          <td class="paramtype">const cv::Mat &amp;&#160;</td>
          <td class="paramname"> <em>tau</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; <a class="el" href="structaction_record.html">actionRecord</a> &gt; &amp;&#160;</td>
          <td class="paramname"> <em>history</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>decisionTimeStamp</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">cv::Mat &amp;&#160;</td>
          <td class="paramname"> <em>action</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Make a decision about how to look at something, taking the dynamics model and a history of eye-movements into account. Only use this command if you have trained the InternalMotionModel's dynamics model, and are making decisions about where to move the eyes on a sub-second timescale. </p>
<p>The desired transform (tau) is a desired pixel translation. When using retinal coordinates, the translation 0, 0 is the central pixel of the current visual field.</p>
<p>To correct for system dynamics, <a class="el" href="structaction_record.html" title="Auxilliary Tool: A data structure that keeps track of a vector-valued action, and a time at which tha...">actionRecord</a> is a history of the eye-movements you have sent, and the times you sent them.</p>
<p>decisionTimeStamp is a record of when the visual information used to make the current saccade was generated. For example, if you are tracking a face with a face-finder, decsionTimeStamp should be a timeStamp at which the image was acquired, before the face finder started running. This is because by the time the face finder finishes running, the robot's eyes are likely to be in a different place than when the image was acquired, and the dynamics model needs to correct for this.</p>
<p>The result is passed by reference back through the parameter action. This is the command you should send to the robot servos.</p>
<p>The units of tau are pixel-based, with respect to the original image size used for training the motion model. For example, if the model was trained with images of size 320x240, and you want to look at the top left of the visual field, then tau should be (160,-120). Note that you are responsible for making this be the case. It's common to have the runtime visual field be higher resolution than that used in training, for example 640x480. In this case, you would want to scale your pixel units by a factor of two. To know what your particular scaling factor should be, call "getEyeMovementUnitsSize()" to find the size of image that the <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a> was originally trained with, and scale your transforms accordingly before calling recommendActionForDesiredTransform. </p>

</div>
</div>
<a class="anchor" id="a695fc8035de43f47c1b8c22f91543480"></a><!-- doxytag: member="InternalMotionModel::setMuEtaPrior" ref="a695fc8035de43f47c1b8c22f91543480" args="(double val=0.5)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setMuEtaPrior </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>val</em> = <code>0.5</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the mean of Lambda intensity estimates to all be a specified value. Eta is an estimate of which areas of the world change quickly or slowly (have high motion). </p>
<p>The default value of 0.5 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. </p>

</div>
</div>
<a class="anchor" id="a28e49fde234e38a0cf1b9eda90c8d3c4"></a><!-- doxytag: member="InternalMotionModel::setMuLambdaPrior" ref="a28e49fde234e38a0cf1b9eda90c8d3c4" args="(double val=0.5)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setMuLambdaPrior </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>val</em> = <code>0.5</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the mean of Lambda intensity estimates to all be a specified value. Lambda is the estimate of the world appearance. </p>
<p>The default value of 0.5 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. </p>

</div>
</div>
<a class="anchor" id="a5b65b30ef8a49b820acabed7a6070577"></a><!-- doxytag: member="InternalMotionModel::setNumDynamicsModels" ref="a5b65b30ef8a49b820acabed7a6070577" args="(size_t num)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setNumDynamicsModels </td>
          <td>(</td>
          <td class="paramtype">size_t&#160;</td>
          <td class="paramname"> <em>num</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the number of time points used for estimating the Dynamics Model. This resets the estimate of the motion parameters. </p>
<p>The dynamics models are important for high speed tracking, but they are not necessary for the basic behavior of the system to work. The dynamics model is trained by having separate time-points at frame-rate intervals after an eye movement. For example, if the camera is 30FPS, then the learning protocal would be to issue an eye movement, collect the next 30 frames, and then call updateModelMAP(image[i], action, i) for each of the 30 images in turn. </p>

</div>
</div>
<a class="anchor" id="ab48e6a8352d7e5f06e6b3b7e64b7e1f7"></a><!-- doxytag: member="InternalMotionModel::setQSquaredEta" ref="ab48e6a8352d7e5f06e6b3b7e64b7e1f7" args="(double std=0.01)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setQSquaredEta </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>std</em> = <code>0.01</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the camera noise variance (how much do you trust an unreliable sensor?). Eta is an estimate of which areas of the world change quickly or slowly (have high motion). </p>
<p>The default value of 0.01 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. Note that 0.01 variance means standard deviation of 0.1. This is relatively high, and asserts that the current field of view is an unreliable representation of the world's appearance. This helps filter out abberations caused by rapid motion through the visual field. </p>

</div>
</div>
<a class="anchor" id="ac1c7eacd0995e054d2ca17c96f12a30c"></a><!-- doxytag: member="InternalMotionModel::setQSquaredLambda" ref="ac1c7eacd0995e054d2ca17c96f12a30c" args="(double std=0.01)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setQSquaredLambda </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>std</em> = <code>0.01</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the camera noise variance (how much do you trust an unreliable sensor?). Lambda is the estimate of the world appearance. </p>
<p>The default value of 0.01 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. Note that 0.01 variance means standard deviation of 0.1. This is relatively high, and asserts that the current field of view is an unreliable representation of the world's appearance. This helps filter out abberations caused by rapid motion through the visual field. </p>

</div>
</div>
<a class="anchor" id="aa44f0646d9b3b5093fc0056257b22d93"></a><!-- doxytag: member="InternalMotionModel::setRSquaredEta" ref="aa44f0646d9b3b5093fc0056257b22d93" args="(double std=0.0001)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setRSquaredEta </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>std</em> = <code>0.0001</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the pixel drift variance (how much do pixels change their value over time?). Eta is an estimate of which areas of the world change quickly or slowly (have high motion). </p>
<p>The default value of 0.0001 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. Note that 0.0001 variance means standard deviation of 0.01. This is relatively low, and asserts that the appearance of the world will be mostly static. </p>

</div>
</div>
<a class="anchor" id="abfa0f7139626d149e371b62c7393b4b4"></a><!-- doxytag: member="InternalMotionModel::setRSquaredLambda" ref="abfa0f7139626d149e371b62c7393b4b4" args="(double std=0.0025)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setRSquaredLambda </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>std</em> = <code>0.0025</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the pixel drift variance (how much do pixels change their value over time?). Lambda is the estimate of the world appearance. </p>
<p>The default value of 0.0001 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. Note that 0.0025 variance means standard deviation of 0.05. This means the pixel values from frame to frame are expected to change by about 5% (rarely more than 15%). </p>

</div>
</div>
<a class="anchor" id="ad73e20fe0f426d5872e1b1eccd1f9642"></a><!-- doxytag: member="InternalMotionModel::setSigmaSquaredEtaPrior" ref="ad73e20fe0f426d5872e1b1eccd1f9642" args="(double val=0.25)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setSigmaSquaredEtaPrior </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>val</em> = <code>0.25</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the variance of the Image intensity estimates to all be a specified value. Eta is an estimate of which areas of the world change quickly or slowly (have high motion). </p>
<p>The default value of 0.25 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. Note that 0.25 variance means standard deviation of 0.5. </p>

</div>
</div>
<a class="anchor" id="a5ed87ca576afece3bbd17fde5b136f4c"></a><!-- doxytag: member="InternalMotionModel::setSigmaSquaredLambdaPrior" ref="a5ed87ca576afece3bbd17fde5b136f4c" args="(double val=0.25)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setSigmaSquaredLambdaPrior </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>val</em> = <code>0.25</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Set the variance of the Image intensity estimates to all be a specified value. Lambda is the estimate of the world appearance. </p>
<p>The default value of 0.25 reflects the assumption that pixel intensities are between 0 and 1. If your images uses a different range of (floating point) values, you may want to set a different mean. Note that 0.25 variance means standard deviation of 0.5. </p>

</div>
</div>
<a class="anchor" id="a7386ed570844d39596ae4eb2d439ced9"></a><!-- doxytag: member="InternalMotionModel::setUseLearnedInverseModel" ref="a7386ed570844d39596ae4eb2d439ced9" args="(int yesorno=0)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setUseLearnedInverseModel </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>yesorno</em> = <code>0</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>EXPERIMENTAL: Use a directly learned inverse model for computing <a class="el" href="class_internal_motion_model.html#a0ed0f0563e35ecd87eb1d56bcea5b0c0" title="Make a decision about how to look at something. This command is sufficient for low-speed tracking...">recommendActionForDesiredTransform()</a>. You probably should not use this. </p>
<p>It can be enabled using setUseLearnedInverseModel(1), but it should not be. It is unreliable when there are more motors than translation dimensions, and the computed inverse method used by default is much better at taking the reliability of the individaul motors into account. </p>

</div>
</div>
<a class="anchor" id="ab001a2562119eb021952df35b0c0f88f"></a><!-- doxytag: member="InternalMotionModel::setUseRetinalCoordinates" ref="ab001a2562119eb021952df35b0c0f88f" args="(int yesorno=1)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::setUseRetinalCoordinates </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>yesorno</em> = <code>1</code>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Sets the coordinate frame to World Coordinates (0) or Retinal Coordinates (Non-0). By default, retinal coordinates are used. </p>
<p>In retinal coordinates, the center of the <a class="el" href="class_image_kalman_filter.html" title="Auxilliary Tool: A Collection of Kalman Filters for estimating the appearance of an image...">ImageKalmanFilter</a> is always colocated with the center of the most recent observation. Actions are interpreted as being relevant to this central location.</p>
<p>In world coordinates, actions are interpreted as being relative to the servo reference frame rather than the camera reference frame.</p>
<p>The <a class="el" href="class_internal_motion_model.html" title="  Machine Perception Primitive:   An implementation of a model that allows a robot to learn about its...">InternalMotionModel</a> is much more robust in a retinal coordinate frame than in a servo coordinate frame. It is not recommended to set this to 0. </p>

</div>
</div>
<a class="anchor" id="a82524c549daab8c92569cf5f78709d23"></a><!-- doxytag: member="InternalMotionModel::updateModelMAP" ref="a82524c549daab8c92569cf5f78709d23" args="(IplImage *seenImage, const cv::Mat &amp;action, int frameNum=&#45;1, double frameTime=0)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void InternalMotionModel::updateModelMAP </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>seenImage</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const cv::Mat &amp;&#160;</td>
          <td class="paramname"> <em>action</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>frameNum</em> = <code>-1</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>frameTime</em> = <code>0</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>This is the main training function. It finds the most likely offset of the seenImage in the representation of remembered visual history, and uses that, along with the action taken, to train the Motion Model. Optionally, frameNum and frameTime can be supplied to train the DynamicsModel. </p>
<p>The process of updating the model has the following steps:</p>
<ul>
<li>cv::Mat MAPTransform; </li>
<li>findMaxLikelihoodTransformFaster(seenImage, action, MAPTransform, frameNum); </li>
<li>updateModel(seenImage, action, MAPTransform, frameNum); </li>
<li>if (frameNum &gt; -1) updateFrameTime(frameNum, frameTime);</li>
</ul>
<p>You can call these functions instead yourself, if you want to inspect the process. Alternatively, you can call <a class="el" href="class_internal_motion_model.html#a486fab952663ef8ebd415520d09b815f" title="Find the transform (x,y translation) that maximizes obsLogLikelihood. The result is passed back throu...">findMaxLikelihoodTransform()</a>, which is slower but more accurate than <a class="el" href="class_internal_motion_model.html#a3f3fa4091055991ef2edd4b3ce2c544d" title="Find the transform (x,y translation) that approximately maximizes obsLogLikelihood. The result is passed back through the reference to the matrix tau. This function is much faster (at a negligible cost to accuracy) than findMaxLikelihoodTransform().">findMaxLikelihoodTransformFaster()</a>. </p>

</div>
</div>
<hr/><h2>Member Data Documentation</h2>
<a class="anchor" id="a27baa5ff19b247c4f06a752d693824ed"></a><!-- doxytag: member="InternalMotionModel::alphaDynamics" ref="a27baa5ff19b247c4f06a752d693824ed" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a>&gt; <a class="el" href="class_internal_motion_model.html#a27baa5ff19b247c4f06a752d693824ed">InternalMotionModel::alphaDynamics</a></td>
        </tr>
      </table>
</div>
<div class="memdoc">
<p>An collection of motor gains used to estimate a family of entire trajectories learned by the robot. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. </p>

</div>
</div>
<a class="anchor" id="ad39cc9e30223ac06b5109cfb3a78364b"></a><!-- doxytag: member="InternalMotionModel::nuDynamics" ref="ad39cc9e30223ac06b5109cfb3a78364b" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;<a class="el" href="class_matrix_kalman_filter.html">MatrixKalmanFilter</a>&gt; <a class="el" href="class_internal_motion_model.html#ad39cc9e30223ac06b5109cfb3a78364b">InternalMotionModel::nuDynamics</a></td>
        </tr>
      </table>
</div>
<div class="memdoc">
<p>An collection of motor reliability estimates used to estimate a family of entire trajectories learned by the robot. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. </p>

</div>
</div>
<a class="anchor" id="a41c762614e142ce0dc2766456d2f9d1a"></a><!-- doxytag: member="InternalMotionModel::timeStamps" ref="a41c762614e142ce0dc2766456d2f9d1a" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt;double&gt; <a class="el" href="class_internal_motion_model.html#a41c762614e142ce0dc2766456d2f9d1a">InternalMotionModel::timeStamps</a></td>
        </tr>
      </table>
</div>
<div class="memdoc">
<p>An collection of timepoints for the motor trajectories learned by the robot. These are important for planning eye-movements on a sub-second timescale. This is the actual underlying estimate data. It is exposed for convenient inspection. Don't modify it. </p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>/Users/nick/projects/NickThesis/Code/OpenCV/src/InternalMotionModel.h</li>
<li>/Users/nick/projects/NickThesis/Code/OpenCV/src/InternalMotionModel.cpp</li>
</ul>
</div>
<hr class="footer"/><address class="footer"><small>Generated on Thu Dec 9 2010 16:05:49 for Nick's Machine Perception Toolbox by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.2 </small></address>
</body>
</html>
