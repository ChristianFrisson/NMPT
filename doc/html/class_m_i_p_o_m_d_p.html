<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Nick&#39;s Machine Perception Toolbox: MIPOMDP Class Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.2 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul class="tablist">
      <li><a href="main.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li class="current"><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
  <div class="tabs2">
    <ul class="tablist">
      <li><a href="annotated.html"><span>Class&#160;List</span></a></li>
      <li><a href="hierarchy.html"><span>Class&#160;Hierarchy</span></a></li>
      <li><a href="functions.html"><span>Class&#160;Members</span></a></li>
    </ul>
  </div>
</div>
<div class="header">
  <div class="summary">
<a href="#pub-methods">Public Member Functions</a> &#124;
<a href="#pub-static-methods">Static Public Member Functions</a> &#124;
<a href="#pub-attribs">Public Attributes</a>  </div>
  <div class="headertitle">
<h1>MIPOMDP Class Reference<br/>
<small>
[<a class="el" href="group___m_p_group.html">Machine Perception Primitives</a>]</small>
</h1>  </div>
</div>
<div class="contents">
<!-- doxytag: class="MIPOMDP" -->
<p><code> <b> Machine Perception Primitive: </b> </code> An implementation of the "Multinomial IPOMDP" algorithm from Butko and Movellan, 2009 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>).  
<a href="#_details">More...</a></p>

<p><code>#include &lt;MIPOMDP.h&gt;</code></p>

<p>Inherited by MOMIPOMDP.</p>
<!-- startSectionHeader --><div class="dynheader">
Collaboration diagram for MIPOMDP:<!-- endSectionHeader --></div>
<!-- startSectionSummary --><!-- endSectionSummary --><!-- startSectionContent --><div class="dyncontent">
<div class="center"><img src="class_m_i_p_o_m_d_p__coll__graph.png" border="0" usemap="#_m_i_p_o_m_d_p_coll__map" alt="Collaboration graph"/></div>
<map name="_m_i_p_o_m_d_p_coll__map" id="_m_i_p_o_m_d_p_coll__map">
<area shape="rect" id="node2" href="class_convolutional_logistic_policy.html" title="Auxilliary Tool: A class for implementing convolutional logistic policies, which were first used in t..." alt="" coords="5,181,197,211"/><area shape="rect" id="node4" href="class_multinomial_observation_model.html" title="Auxilliary Tool: Multionomial Observation Model, as described in Butko and Movellan, CVPR 2009 (see Related Publications). This data structure maintains the mapping between object detector output and the probability that the target object is located at every grid&#45;cell." alt="" coords="221,181,432,211"/><area shape="rect" id="node6" href="class_image_patch_pyramid.html" title="Auxilliary Tool: The main data structure for the MIPOMDP algorithm. It is reponsible for generating t..." alt="" coords="456,181,605,211"/><area shape="rect" id="node8" href="class_open_c_v_haar_detector.html" title="Auxilliary Tool: A specific object detector that uses OpenCV&#39;s Haar Cascade Classifier to detect ..." alt="" coords="451,85,611,115"/><area shape="rect" id="node10" href="class_object_detector.html" title="Auxilliary Tool: A virtual class for providing the skeleton for specific object detectors." alt="" coords="472,5,589,35"/></map>
<center><span class="legend">[<a target="top" href="graph_legend.html">legend</a>]</span></center><!-- endSectionContent --></div>

<p><a href="class_m_i_p_o_m_d_p-members.html">List of all members.</a></p>
<table class="memberdecls">
<tr><td colspan="2"><h2><a name="pub-methods"></a>
Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a6fdb71ec3a67b0d49c88d7df8364b28a">MIPOMDP</a> (CvSize inputImageSize, CvSize subImageSize, CvSize gridSize, int numSubImages, CvMat *subImageGridPoints, const char *haarDetectorXMLFile)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Main Constructor: Manually create an <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>.  <a href="#a6fdb71ec3a67b0d49c88d7df8364b28a"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="adc17b57ddc54fb7c0031e49074cac91b"></a><!-- doxytag: member="MIPOMDP::MIPOMDP" ref="adc17b57ddc54fb7c0031e49074cac91b" args="()" -->
&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#adc17b57ddc54fb7c0031e49074cac91b">MIPOMDP</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Default Constructor: Create an <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> with the same properties as the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> used in Butko and Movellan, CVPR 2009 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). NOTE: This requires "data/haarcascade_frontalface_alt2.xml" to be a haar-detector file that exists. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a4bf56b7fbd4b53491bf90225c74f190e"></a><!-- doxytag: member="MIPOMDP::~MIPOMDP" ref="a4bf56b7fbd4b53491bf90225c74f190e" args="()" -->
virtual&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a4bf56b7fbd4b53491bf90225c74f190e">~MIPOMDP</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Default Destructor: Deallocates all memory associated with the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a698fad3f28257961f0f8b749e0600696">saveToFile</a> (const char *filename)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Save <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> Data: Save data about the structure of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> so that it can be persist past the current run of the program. This includes details about the structure of the IPP, the object detector used, the parameters of the multionmial observation model, the policy used. It does not include details about the state of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> (the current location of the target), but rather the properties of the model.  <a href="#a698fad3f28257961f0f8b749e0600696"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="afee47b14c3ff9b3c19208222cf2d82a5"></a><!-- doxytag: member="MIPOMDP::resetPrior" ref="afee47b14c3ff9b3c19208222cf2d82a5" args="()" -->
void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#afee47b14c3ff9b3c19208222cf2d82a5">resetPrior</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Reset the belief about the location of the object to be uniform over space (complete uncertainty). <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a3e1138e295cbc03764334319661fdfed">setTargetCanMove</a> (int flag)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Tell <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> whether there is a possiblity that the target can move. When searching a static frame, set this to 0. When searching sequential frames of a movie, set this to 1.  <a href="#a3e1138e295cbc03764334319661fdfed"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#aabd5a29a325e80e5a6cc48005f3ed54a">getTargetCanMove</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Figure out whether the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> expects that the target can move.  <a href="#aabd5a29a325e80e5a6cc48005f3ed54a"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvSize&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a3da2996a603183f24c6c086a43ef380f">getGridSize</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Find the shape of the grid that forms the basis for the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> state space and action space.  <a href="#a3da2996a603183f24c6c086a43ef380f"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ab2b040022b66fa353784664801653ae2">recommendSearchPointForCurrentBelief</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Given the current point, where does the MIPOMDP's Convolutional logistic policy recommend looking? Since the CLP (usually) gives stochastic output, this will not always return the same result, even given the same belief state.  <a href="#ab2b040022b66fa353784664801653ae2"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a277002b9f97a0fcd3a0399a53ee371e0">getProb</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Get an estimate of the probability that the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> knows the exactly correct location of the target.  <a href="#a277002b9f97a0fcd3a0399a53ee371e0"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#aa569fe902268b6b18b629f0db6ac796d">getReward</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: The certainty (information-reward) about the location of the target, i.e. the mutual information (minus a constant) between all previous actions/observations and the target location.  <a href="#aa569fe902268b6b18b629f0db6ac796d"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a6640e23113509d653220d83adcbb92f8">getMostLikelyTargetLocation</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Find the grid-cell that is most likely to be the location of the target.  <a href="#a6640e23113509d653220d83adcbb92f8"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#aa47a92982c89f26c9f844112aa0aae63">searchNewFrame</a> (IplImage *grayFrame)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a location recommended by the Convolutional Logistic Policy, and update belief based on what was found.  <a href="#aa47a92982c89f26c9f844112aa0aae63"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a966a7855d80548dd1d132ca048902817">searchNewFrameAtGridPoint</a> (IplImage *grayFrame, CvPoint searchPoint)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a location decided by the calling program. and update belief based on what was found.  <a href="#a966a7855d80548dd1d132ca048902817"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">virtual CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a9f23896854917385d6f22a8085927778">searchFrameAtGridPoint</a> (IplImage *grayFrame, CvPoint searchPoint)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a location recommended by the calling program, and update belief based on what was found.  <a href="#a9f23896854917385d6f22a8085927778"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#abc98c1d7f36b6628ff5d5faea2d393ea">searchFrameUntilConfident</a> (IplImage *grayFrame, double confidenceThresh)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a sequence of fixation points recommended by the CLP, and update belief based on what was found. Employs an early-stop criterion of the first repeat fixation. Otherwise, stops at when confidence in the target location reaches a maximum value.  <a href="#abc98c1d7f36b6628ff5d5faea2d393ea"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a146c94b9dd492754f7d1fea552495b6e">searchFrameRandomlyUntilConfident</a> (IplImage *grayFrame, double confidenceThresh)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a sequence of fixation points chosen randomly, and update belief based on what was found. Employs an early-stop criterion of the first repeat fixation. Otherwise, stops at when confidence in the target location reaches a maximum value.  <a href="#a146c94b9dd492754f7d1fea552495b6e"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a77eb111c1e07f3c545dce72aa941a18a">searchFrameForNFixations</a> (IplImage *grayFrame, int numfixations)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a sequence of fixation points recommended by the CLP, and update belief based on what was found. Fixates the specified number of times (no early stopping is used).  <a href="#a77eb111c1e07f3c545dce72aa941a18a"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#afaf9043c76d04e4d32b5e231e11d1cfd">searchFrameForNFixationsOpenLoop</a> (IplImage *grayFrame, int numfixations, int OLPolicyType)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a sequence of fixation points recommended by an open loop fixation policy, and update belief based on what was found. Fixates the specified number of times (no early stopping is used).  <a href="#afaf9043c76d04e4d32b5e231e11d1cfd"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#af0e80e312917848db6cec3de39614f19">searchFrameForNFixationsAndVisualize</a> (IplImage *grayFrame, int numfixations, const char *window, int msec_wait)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at a sequence of fixation points recommended by the CLP, and update belief based on what was found. Fixates the specified number of times (no early stopping is used). Visualize the process in a provided OpenCV UI Window, at a specified frame rate.  <a href="#af0e80e312917848db6cec3de39614f19"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ad6876b27bd62cbc314673e2ae33d7816">searchFrameAtAllGridPoints</a> (IplImage *grayFrame)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Search an image (or frame of video) at every grid-point (no early stopping is used).  <a href="#ad6876b27bd62cbc314673e2ae33d7816"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a0b7501d44e0c9f3c6076d75b7b0d5bfa">searchHighResImage</a> (IplImage *grayFrame)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Search function: Apply the Object Detector to the entire input image.  <a href="#a0b7501d44e0c9f3c6076d75b7b0d5bfa"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a68a65f637a16ddef96fc9d99b46cd289">changeInputImageSize</a> (CvSize newInputSize)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Change the size of the input image and the downsampled image patches. Omitting a newSubImageSize causes the smallest-used-scale to have a 1-1 pixel mapping with the downsampled image patch -- i.e. information is not lost in the smallest scale.  <a href="#a68a65f637a16ddef96fc9d99b46cd289"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a0bd70a063d6a857518f18e9f8d2728fc">changeInputImageSize</a> (CvSize newInputSize, CvSize newSubImageSize)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Change the size of the input image and the downsampled image patches.  <a href="#a0bd70a063d6a857518f18e9f8d2728fc"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">int&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#af67d5135a52799f360d4e321c9452c11">getNumScales</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: The total number of levels that the IP Pyramid has.  <a href="#af67d5135a52799f360d4e321c9452c11"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="add274461e0232a308a9c69c2a3940998"></a><!-- doxytag: member="MIPOMDP::gridPointForPixel" ref="add274461e0232a308a9c69c2a3940998" args="(CvPoint pixel)" -->
CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#add274461e0232a308a9c69c2a3940998">gridPointForPixel</a> (CvPoint pixel)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Map a pixel location in the original image into a grid-cell. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="ac4df6f98c8c10f09c8b370459c8d7788"></a><!-- doxytag: member="MIPOMDP::pixelForGridPoint" ref="ac4df6f98c8c10f09c8b370459c8d7788" args="(CvPoint gridPoint)" -->
CvPoint&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ac4df6f98c8c10f09c8b370459c8d7788">pixelForGridPoint</a> (CvPoint gridPoint)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Find the pixel in the original image that is in the center of a grid-cell. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ab51fd25d10e5dfda3d67a32f46c7c6a7">setGeneratePreview</a> (int flag)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Turns on/off the code that modifies foveaRepresentation to visualize the process of fixating.  <a href="#ab51fd25d10e5dfda3d67a32f46c7c6a7"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ac5d779f6f0296fabb64ee64eaa656a03">setMinSize</a> (CvSize minsize)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Set the minimum allowed subImageSize.  <a href="#ac5d779f6f0296fabb64ee64eaa656a03"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a798586d237354b3106cfc945d38aceb3">useSameFrameOptimizations</a> (int flag)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Set whether same-frame optimizations are being used.  <a href="#a798586d237354b3106cfc945d38aceb3"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#abff0e408d5eb354ce12902937b19c3c5">saveVisualization</a> (IplImage *grayFrame, CvPoint searchPoint, const char *base_filename)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Save a variety of visual representations of the process of fixating with an IPP to image files.  <a href="#abff0e408d5eb354ce12902937b19c3c5"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top"><a class="anchor" id="a03a0bd28d3eefa9187dda9bc3e1931b7"></a><!-- doxytag: member="MIPOMDP::getCounts" ref="a03a0bd28d3eefa9187dda9bc3e1931b7" args="()" -->
IplImage *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a03a0bd28d3eefa9187dda9bc3e1931b7">getCounts</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with IPP: Get the count of faces found in each grid cell after the last search call. <br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a0f6fd2e5c38bcc8d4565d5925f884ed0">setHaarCascadeScaleFactor</a> (double factor)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with Object Detector: Sets the factor by which the image-patch-search-scale is increased. Should be greater than 1. By default, the scale factor is 1.1, meaning that faces are searched for at sizes that increase by 10%.  <a href="#a0f6fd2e5c38bcc8d4565d5925f884ed0"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a0b3c9a261086b2591814522bf1716a01">setHaarCascadeMinSize</a> (int size)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with Object Detector: Sets the minimum patch size at which the classifier searches for the object. By default, this is 0, meaning that the smallest size appropriate to XML file is used. In the case of the frontal face detector provided, this happens to be 20x20 pixels.  <a href="#a0b3c9a261086b2591814522bf1716a01"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a11b6d6969860683c89170bfd6a85337f">trainObservationModel</a> (<a class="el" href="class_image_data_set.html">ImageDataSet</a> *trainingSet)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with Observation Model: Fit a multinomial observation model by recording the object detector output at different fixation points, given known face locations contained in an <a class="el" href="class_image_data_set.html" title="Auxilliary Tool: A data structure for maintaining a list of image files and image labels to facilitat...">ImageDataSet</a>.  <a href="#a11b6d6969860683c89170bfd6a85337f"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a98eda4c7b5c49b3fd8e8417229f423bf">addDataToObservationModel</a> (<a class="el" href="class_image_data_set.html">ImageDataSet</a> *trainingSet)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with Observation Model: Add data to an existing multinomial observation model by recording the object detector output at different fixation points, given known face locations contained in an <a class="el" href="class_image_data_set.html" title="Auxilliary Tool: A data structure for maintaining a list of image files and image labels to facilitat...">ImageDataSet</a>.  <a href="#a98eda4c7b5c49b3fd8e8417229f423bf"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a08e5b0ebd751ee3a0a6331b33bd9c7c7">resetModel</a> ()</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with Observation Model: Reset the experience-counts used to estimate the multinomial parameters. Sets all counts to 1.  <a href="#a08e5b0ebd751ee3a0a6331b33bd9c7c7"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ae23411449219982317494348b6196946">combineModels</a> (<a class="el" href="class_m_i_p_o_m_d_p.html">MIPOMDP</a> *otherPomdp)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with Observation Model: Combine the evidence from two MIPOMDPs' multinomial observation models (merge their counts and subtract off the extra priors).  <a href="#ae23411449219982317494348b6196946"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#af742b9e8b1c5f6b915eb7cedca8f9dfb">setPolicy</a> (int policyNumber)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with CLP: Tell the CLP what kind of convolution policy to use in the future.  <a href="#af742b9e8b1c5f6b915eb7cedca8f9dfb"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ab7119b3aa1176b7119fa2fa7af2b3814">setHeuristicPolicyParameters</a> (double softmaxGain, double boxSize)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with CLP: Tell the CLP the shape of the convolution kernel to use.  <a href="#ab7119b3aa1176b7119fa2fa7af2b3814"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a8431c67294dee47ba23a24f8838cab91">setObjectDetectorSource</a> (std::string newFileName)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Change the file used by the object detector for doing detecting. This is critical if a weights file is located at an absolute path that may have changed from training time.  <a href="#a8431c67294dee47ba23a24f8838cab91"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pub-static-methods"></a>
Static Public Member Functions</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">static <a class="el" href="class_m_i_p_o_m_d_p.html">MIPOMDP</a> *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#ae64349c0e0ce3da39be5c28ff974b8ce">loadFromFile</a> (const char *filename)</td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Load a saved <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> from a file. The file should have been saved via the <a class="el" href="class_m_i_p_o_m_d_p.html#a698fad3f28257961f0f8b749e0600696" title="Save MIPOMDP Data: Save data about the structure of the MIPOMDP so that it can be persist past the cu...">saveToFile()</a> method.  <a href="#ae64349c0e0ce3da39be5c28ff974b8ce"></a><br/></td></tr>
<tr><td colspan="2"><h2><a name="pub-attribs"></a>
Public Attributes</h2></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">IplImage *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a240fdf3ebddc3870bd8d00bd6a3344bb">currentBelief</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Access the current belief distribution directly. Changes each time one of the search functions is called.  <a href="#a240fdf3ebddc3870bd8d00bd6a3344bb"></a><br/></td></tr>
<tr><td class="memItemLeft" align="right" valign="top">IplImage *&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="class_m_i_p_o_m_d_p.html#a2a765ad1957d673ffaa0d6e69f0bb410">foveaRepresentation</a></td></tr>
<tr><td class="mdescLeft">&#160;</td><td class="mdescRight">Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: A visually informative representation of the IPP Foveal represention.  <a href="#a2a765ad1957d673ffaa0d6e69f0bb410"></a><br/></td></tr>
</table>
<hr/><a name="_details"></a><h2>Detailed Description</h2>
<p><code> <b> Machine Perception Primitive: </b> </code> An implementation of the "Multinomial IPOMDP" algorithm from Butko and Movellan, 2009 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). </p>
<dl class="author"><dt><b>Author:</b></dt><dd>Nicholas Butko </dd></dl>
<dl class="date"><dt><b>Date:</b></dt><dd>2010 </dd></dl>
<dl class="version"><dt><b>Version:</b></dt><dd>0.4 </dd></dl>
<hr/><h2>Constructor &amp; Destructor Documentation</h2>
<a class="anchor" id="a6fdb71ec3a67b0d49c88d7df8364b28a"></a><!-- doxytag: member="MIPOMDP::MIPOMDP" ref="a6fdb71ec3a67b0d49c88d7df8364b28a" args="(CvSize inputImageSize, CvSize subImageSize, CvSize gridSize, int numSubImages, CvMat *subImageGridPoints, const char *haarDetectorXMLFile)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">MIPOMDP::MIPOMDP </td>
          <td>(</td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>inputImageSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>subImageSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>gridSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>numSubImages</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvMat *&#160;</td>
          <td class="paramname"> <em>subImageGridPoints</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"> <em>haarDetectorXMLFile</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Main Constructor: Manually create an <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>. </p>
<p>An <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> needs to know how big the images it receives are, to what smaller size it should scale each image patch, the size (height / width) of the grid-cell tiling of the image, how many image patches there will be, the size (height/width) in grid-cells of each image patch, and what object detector to apply.</p>
<p>In order to be able to properly save and load object detectors, the class of the object detector must be specified in the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>. To use an object detector other than <a class="el" href="class_open_c_v_haar_detector.html" title="Auxilliary Tool: A specific object detector that uses OpenCV&#39;s Haar Cascade Classifier to detect ...">OpenCVHaarDetector</a>, replace "OpenCVHaarDetector" in MIPOMDP.h and MIPOMDP.cpp, and recompile the NMPT library. (<a class="el" href="class_m_i_p_o_m_d_p.html#a0f6fd2e5c38bcc8d4565d5925f884ed0" title="Interface with Object Detector: Sets the factor by which the image-patch-search-scale is increased...">setHaarCascadeScaleFactor()</a> and <a class="el" href="class_m_i_p_o_m_d_p.html#a0b3c9a261086b2591814522bf1716a01" title="Interface with Object Detector: Sets the minimum patch size at which the classifier searches for the ...">setHaarCascadeMinSize()</a> will also need to be removed, or implemented in your own object detector for successful compilation).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">inputImageSize</td><td>The size of the images that will be given to the IPP to turn into <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> observations. This allocates memory for underlying data, but it can be changed easily later if needed without recreating the object by calling the changeInputSize() functions.</td></tr>
    <tr><td class="paramname">subImageSize</td><td>The common size to which all image patches will be reduced, creating the foveation effect. The smaller subImageSize is, the faster search is, and the more extreme the effect of foveation. This allocates memory for underlying data, but it can be changed easily later if needed without recreating the object by calling the changeInputSize() functions.</td></tr>
    <tr><td class="paramname">gridSize</td><td>Size of the discretization of the image. The number of POMDP states is the product of the demensions of this size (e.g. 21x21).</td></tr>
    <tr><td class="paramname">numSubImages</td><td>Number of Patches in the Image Patch Pyramid.</td></tr>
    <tr><td class="paramname">subImageGridPoints</td><td>A matrix that describes the size and shape of each level (patch) of the IP Pyramid. This must be a matrix with size [numSubImages x 2]. Each row contains the width and height of the corresponding levels. These should be in order of *decreasing* size, so that the largest Image Patch is first. For example, in Butko and Movellan CVPR 2009, we used [21 21; 15 15; 9 9; 3 3]. Finaly, note that it is not necessary that the largest patch cover the entire image. However, when the largest patch is the same size as grid-cell-matrix, special optimizations become available that reduce the complexity of the algorithm when the same image, or same frame of video, is fixated multiple times.</td></tr>
    <tr><td class="paramname">haarDetectorXMLFile</td><td>A file that was saved as the result of using OpenCV's haar-detector training facilities. In order to be able to properly save and load object detectors, the class of the object detector must be specified in the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>. To use an object detector other than <a class="el" href="class_open_c_v_haar_detector.html" title="Auxilliary Tool: A specific object detector that uses OpenCV&#39;s Haar Cascade Classifier to detect ...">OpenCVHaarDetector</a>, simply replace "OpenCVHaarDetector" in MIPOMDP.h and MIPOMDP.cpp, and recompile the NMPT library. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<hr/><h2>Member Function Documentation</h2>
<a class="anchor" id="a98eda4c7b5c49b3fd8e8417229f423bf"></a><!-- doxytag: member="MIPOMDP::addDataToObservationModel" ref="a98eda4c7b5c49b3fd8e8417229f423bf" args="(ImageDataSet *trainingSet)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::addDataToObservationModel </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_image_data_set.html">ImageDataSet</a> *&#160;</td>
          <td class="paramname"> <em>trainingSet</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with Observation Model: Add data to an existing multinomial observation model by recording the object detector output at different fixation points, given known face locations contained in an <a class="el" href="class_image_data_set.html" title="Auxilliary Tool: A data structure for maintaining a list of image files and image labels to facilitat...">ImageDataSet</a>. </p>
<p>Does not reset the current observation model to a uniform prior before tabulating the data.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">trainingSet</td><td>An iamge dataset, consisting of image files and labeled object locations. The first two indices (0, 1) of all labels in the trainingSet should be the width and height (respectively) of the object's center in the image. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a68a65f637a16ddef96fc9d99b46cd289"></a><!-- doxytag: member="MIPOMDP::changeInputImageSize" ref="a68a65f637a16ddef96fc9d99b46cd289" args="(CvSize newInputSize)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::changeInputImageSize </td>
          <td>(</td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>newInputSize</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: Change the size of the input image and the downsampled image patches. Omitting a newSubImageSize causes the smallest-used-scale to have a 1-1 pixel mapping with the downsampled image patch -- i.e. information is not lost in the smallest scale. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">newInputSize</td><td>The size of the next image that will be searched. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a0bd70a063d6a857518f18e9f8d2728fc"></a><!-- doxytag: member="MIPOMDP::changeInputImageSize" ref="a0bd70a063d6a857518f18e9f8d2728fc" args="(CvSize newInputSize, CvSize newSubImageSize)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::changeInputImageSize </td>
          <td>(</td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>newInputSize</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>newSubImageSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: Change the size of the input image and the downsampled image patches. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">newInputSize</td><td>The size of the next image that will be searched.</td></tr>
    <tr><td class="paramname">newSubImageSize</td><td>The desired size of the downsampled image patches. If the subImageSize is too small (below getMinSize()), the smallest scale is dropped and subImageSize is scaled up proportionally to the next scale. This process is repeated until subImageSIze is greater than getMinSize(). By default, minSize is 60x40. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ae23411449219982317494348b6196946"></a><!-- doxytag: member="MIPOMDP::combineModels" ref="ae23411449219982317494348b6196946" args="(MIPOMDP *otherPomdp)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::combineModels </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_m_i_p_o_m_d_p.html">MIPOMDP</a> *&#160;</td>
          <td class="paramname"> <em>otherPomdp</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with Observation Model: Combine the evidence from two MIPOMDPs' multinomial observation models (merge their counts and subtract off the extra priors). </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">otherPomdp</td><td>A second <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> with a model that has been fit to different data than this one: We can estimate the model for the combined set of data by simply adding the counts, and subtracting duplicate priors. In this way, we can efficiently compose models fit to different subsets of a larger dataset (e.g. for cross-validation). </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a3da2996a603183f24c6c086a43ef380f"></a><!-- doxytag: member="MIPOMDP::getGridSize" ref="a3da2996a603183f24c6c086a43ef380f" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvSize MIPOMDP::getGridSize </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Find the shape of the grid that forms the basis for the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> state space and action space. </p>
<dl class="return"><dt><b>Returns:</b></dt><dd>The size of the visual grid world, where each location is a potential object location (state) and potential eye-movement (action). </dd></dl>

</div>
</div>
<a class="anchor" id="a6640e23113509d653220d83adcbb92f8"></a><!-- doxytag: member="MIPOMDP::getMostLikelyTargetLocation" ref="a6640e23113509d653220d83adcbb92f8" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::getMostLikelyTargetLocation </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Find the grid-cell that is most likely to be the location of the target. </p>
<dl class="return"><dt><b>Returns:</b></dt><dd>The grid-cell that is most likely to be the location of the target. </dd></dl>

</div>
</div>
<a class="anchor" id="af67d5135a52799f360d4e321c9452c11"></a><!-- doxytag: member="MIPOMDP::getNumScales" ref="af67d5135a52799f360d4e321c9452c11" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int MIPOMDP::getNumScales </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: The total number of levels that the IP Pyramid has. </p>
<p>Note that this may be different from the number of scales that the IP Pyramid is using. If the subImageSize is too small (below getMinSize()), the smallest scale is dropped and subImageSize is scaled up proportionally to the next scale. This process is repeated until subImageSIze is greater than getMinSize(). To find out how many scales that the IPP is using, call getUsedScales(). </p>

</div>
</div>
<a class="anchor" id="a277002b9f97a0fcd3a0399a53ee371e0"></a><!-- doxytag: member="MIPOMDP::getProb" ref="a277002b9f97a0fcd3a0399a53ee371e0" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double MIPOMDP::getProb </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Get an estimate of the probability that the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> knows the exactly correct location of the target. </p>
<p>I.e. what is the probability of being correct given the (MxN) alternative forced choice task, "Where is the target" (where M is the width of the grid, and N is the height).</p>
<dl class="return"><dt><b>Returns:</b></dt><dd>The probability that the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> knows the exactly correct location of the target. </dd></dl>

</div>
</div>
<a class="anchor" id="aa569fe902268b6b18b629f0db6ac796d"></a><!-- doxytag: member="MIPOMDP::getReward" ref="aa569fe902268b6b18b629f0db6ac796d" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double MIPOMDP::getReward </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: The certainty (information-reward) about the location of the target, i.e. the mutual information (minus a constant) between all previous actions/observations and the target location. </p>
<dl class="return"><dt><b>Returns:</b></dt><dd>The information-reward (Sum p*log(p)) of the current belief state. Has range -(1/MN)log(MN):0 (where M is the width of the grid, and N is the height). </dd></dl>

</div>
</div>
<a class="anchor" id="aabd5a29a325e80e5a6cc48005f3ed54a"></a><!-- doxytag: member="MIPOMDP::getTargetCanMove" ref="aabd5a29a325e80e5a6cc48005f3ed54a" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">int MIPOMDP::getTargetCanMove </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Figure out whether the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> expects that the target can move. </p>
<dl class="return"><dt><b>Returns:</b></dt><dd>1 (default) if searching sequential frames of a movie, 0 if searching a static image. </dd></dl>

</div>
</div>
<a class="anchor" id="ae64349c0e0ce3da39be5c28ff974b8ce"></a><!-- doxytag: member="MIPOMDP::loadFromFile" ref="ae64349c0e0ce3da39be5c28ff974b8ce" args="(const char *filename)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname"><a class="el" href="class_m_i_p_o_m_d_p.html">MIPOMDP</a> * MIPOMDP::loadFromFile </td>
          <td>(</td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"> <em>filename</em>&#160;)</td>
          <td><code> [static]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Load a saved <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> from a file. The file should have been saved via the <a class="el" href="class_m_i_p_o_m_d_p.html#a698fad3f28257961f0f8b749e0600696" title="Save MIPOMDP Data: Save data about the structure of the MIPOMDP so that it can be persist past the cu...">saveToFile()</a> method. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">filename</td><td>The name of a file that was saved via the <a class="el" href="class_m_i_p_o_m_d_p.html#a698fad3f28257961f0f8b749e0600696" title="Save MIPOMDP Data: Save data about the structure of the MIPOMDP so that it can be persist past the cu...">saveToFile()</a> method. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ab2b040022b66fa353784664801653ae2"></a><!-- doxytag: member="MIPOMDP::recommendSearchPointForCurrentBelief" ref="ab2b040022b66fa353784664801653ae2" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::recommendSearchPointForCurrentBelief </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Given the current point, where does the MIPOMDP's Convolutional logistic policy recommend looking? Since the CLP (usually) gives stochastic output, this will not always return the same result, even given the same belief state. </p>
<dl class="return"><dt><b>Returns:</b></dt><dd>A grid-cell point that should be good to fixate. </dd></dl>

</div>
</div>
<a class="anchor" id="a08e5b0ebd751ee3a0a6331b33bd9c7c7"></a><!-- doxytag: member="MIPOMDP::resetModel" ref="a08e5b0ebd751ee3a0a6331b33bd9c7c7" args="()" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::resetModel </td>
          <td>(</td>
          <td class="paramname">&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with Observation Model: Reset the experience-counts used to estimate the multinomial parameters. Sets all counts to 1. </p>
<p>The multinomial distribution probabilities are estimated from experience counts. The table of counts forms a dirichlet posterior over the parameters of the multinomials. When the counts are reset to 1, there is a flat prior over multinomial parameters, and all outcomes are seen as equally likely. This is a particularly bad model, and will make it impossible to figure out the location of the face, so only call resetCounts if you're then going to call updateProbTableCounts for enough images to build up a good model. </p>

</div>
</div>
<a class="anchor" id="a698fad3f28257961f0f8b749e0600696"></a><!-- doxytag: member="MIPOMDP::saveToFile" ref="a698fad3f28257961f0f8b749e0600696" args="(const char *filename)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::saveToFile </td>
          <td>(</td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"> <em>filename</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Save <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> Data: Save data about the structure of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> so that it can be persist past the current run of the program. This includes details about the structure of the IPP, the object detector used, the parameters of the multionmial observation model, the policy used. It does not include details about the state of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> (the current location of the target), but rather the properties of the model. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">filename</td><td>The name of the saved file. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="abff0e408d5eb354ce12902937b19c3c5"></a><!-- doxytag: member="MIPOMDP::saveVisualization" ref="abff0e408d5eb354ce12902937b19c3c5" args="(IplImage *grayFrame, CvPoint searchPoint, const char *base_filename)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::saveVisualization </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvPoint&#160;</td>
          <td class="paramname"> <em>searchPoint</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"> <em>base_filename</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: Save a variety of visual representations of the process of fixating with an IPP to image files. </p>
<p>This method saves a series of .png image files, each with a prefix given by base_filename. Images with the following suffix are created: </p>
<ul>
<li>FullInputImage - The full input image contained in grayFrame. </li>
<li>Scale-[0:N] - The down-sampled representation of each image patch. </li>
<li>FoveatedInputImage - A reconstruction of the full image using the donwsampled patches. </li>
<li>FoveatedInputImageWithLooking - Same as above, with white boxes drawn around each scale. </li>
<li>FoveatedInputImageWithGrid - Same as above, but with a grid overlayed showing the grid-cells. </li>
<li>FullInputImageWithGrid - Full image with black rectangles showing grid-cell locations. </li>
<li>FullInputImageWithLooking - Same as above, but with wite boxes drawn around each scale.</li>
</ul>
<p>Additionally, one CSV file is created, suffix "FaceCounts.csv", which records the output of the object detector on the foveated representation in each grid-cell.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. This image should have size inputImageSize, and be of type IPL_DEPTH_8U with a single channel.</td></tr>
    <tr><td class="paramname">searchPoint</td><td>The center of fixation.</td></tr>
    <tr><td class="paramname">base_filename</td><td>All of the files generated by this function will be given this as a prefix. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ad6876b27bd62cbc314673e2ae33d7816"></a><!-- doxytag: member="MIPOMDP::searchFrameAtAllGridPoints" ref="ad6876b27bd62cbc314673e2ae33d7816" args="(IplImage *grayFrame)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameAtAllGridPoints </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at every grid-point (no early stopping is used). </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations on the first saccade, and then that it can use same-frame optimizations on subsequent fixations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="a9f23896854917385d6f22a8085927778"></a><!-- doxytag: member="MIPOMDP::searchFrameAtGridPoint" ref="a9f23896854917385d6f22a8085927778" args="(IplImage *grayFrame, CvPoint searchPoint)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameAtGridPoint </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvPoint&#160;</td>
          <td class="paramname"> <em>searchPoint</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td><code> [virtual]</code></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a location recommended by the calling program, and update belief based on what was found. </p>
<p>Since this is taken to be a repeat of the last frame, the IPP knows that it can use same-frame optimizations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">searchPoint</td><td>The grid location to center the digital fovea for further image processing.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="a77eb111c1e07f3c545dce72aa941a18a"></a><!-- doxytag: member="MIPOMDP::searchFrameForNFixations" ref="a77eb111c1e07f3c545dce72aa941a18a" args="(IplImage *grayFrame, int numfixations)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameForNFixations </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>numfixations</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a sequence of fixation points recommended by the CLP, and update belief based on what was found. Fixates the specified number of times (no early stopping is used). </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations on the first saccade, and then that it can use same-frame optimizations on subsequent fixations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">numfixations</td><td>The number of fixations to apply.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="af0e80e312917848db6cec3de39614f19"></a><!-- doxytag: member="MIPOMDP::searchFrameForNFixationsAndVisualize" ref="af0e80e312917848db6cec3de39614f19" args="(IplImage *grayFrame, int numfixations, const char *window, int msec_wait)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameForNFixationsAndVisualize </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>numfixations</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const char *&#160;</td>
          <td class="paramname"> <em>window</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>msec_wait</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a sequence of fixation points recommended by the CLP, and update belief based on what was found. Fixates the specified number of times (no early stopping is used). Visualize the process in a provided OpenCV UI Window, at a specified frame rate. </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations on the first saccade, and then that it can use same-frame optimizations on subsequent fixations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">numfixations</td><td>The number of fixations to apply.</td></tr>
    <tr><td class="paramname">window</td><td>The string handle (name) of an OpenCV UI Window that you have previously created. Output will displayed in this window.</td></tr>
    <tr><td class="paramname">msec_wait</td><td>Number of milliseconds to wait between fixations so that the process of each fixation can be appreciated visually by the user.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="afaf9043c76d04e4d32b5e231e11d1cfd"></a><!-- doxytag: member="MIPOMDP::searchFrameForNFixationsOpenLoop" ref="afaf9043c76d04e4d32b5e231e11d1cfd" args="(IplImage *grayFrame, int numfixations, int OLPolicyType)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameForNFixationsOpenLoop </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>numfixations</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>OLPolicyType</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a sequence of fixation points recommended by an open loop fixation policy, and update belief based on what was found. Fixates the specified number of times (no early stopping is used). </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations on the first saccade, and then that it can use same-frame optimizations on subsequent fixations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">numfixations</td><td>The number of fixations to apply.</td></tr>
    <tr><td class="paramname">OLPolicyType</td><td>The type of open-loop policy to use. Should be one of OpenLoopPolicy::RANDOM, OpenLoopPolicy::ORDERED, OpenLoopPolicy::SPIRAL.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="a146c94b9dd492754f7d1fea552495b6e"></a><!-- doxytag: member="MIPOMDP::searchFrameRandomlyUntilConfident" ref="a146c94b9dd492754f7d1fea552495b6e" args="(IplImage *grayFrame, double confidenceThresh)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameRandomlyUntilConfident </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>confidenceThresh</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a sequence of fixation points chosen randomly, and update belief based on what was found. Employs an early-stop criterion of the first repeat fixation. Otherwise, stops at when confidence in the target location reaches a maximum value. </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations on the first saccade, and then that it can use same-frame optimizations on subsequent fixations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">confidenceThresh</td><td>Probability that max-location really contains the object before stopping.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="abc98c1d7f36b6628ff5d5faea2d393ea"></a><!-- doxytag: member="MIPOMDP::searchFrameUntilConfident" ref="abc98c1d7f36b6628ff5d5faea2d393ea" args="(IplImage *grayFrame, double confidenceThresh)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchFrameUntilConfident </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>confidenceThresh</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a sequence of fixation points recommended by the CLP, and update belief based on what was found. Employs an early-stop criterion of the first repeat fixation. Otherwise, stops at when confidence in the target location reaches a maximum value. </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations on the first saccade, and then that it can use same-frame optimizations on subsequent fixations (if appropriate).</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">confidenceThresh</td><td>Probability that max-location really contains the object before stopping.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="a0b7501d44e0c9f3c6076d75b7b0d5bfa"></a><!-- doxytag: member="MIPOMDP::searchHighResImage" ref="a0b7501d44e0c9f3c6076d75b7b0d5bfa" args="(IplImage *grayFrame)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchHighResImage </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Apply the Object Detector to the entire input image. </p>
<p>The belief map is not changed by this operation. The count vector of objects found in each grid-cell is set. The location of the grid-cell with the most found objects is returned. The fovea representation is set to contain the entire high-resolution image, overlaid with grid-cells. </p>

</div>
</div>
<a class="anchor" id="aa47a92982c89f26c9f844112aa0aae63"></a><!-- doxytag: member="MIPOMDP::searchNewFrame" ref="aa47a92982c89f26c9f844112aa0aae63" args="(IplImage *grayFrame)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchNewFrame </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a location recommended by the Convolutional Logistic Policy, and update belief based on what was found. </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="a966a7855d80548dd1d132ca048902817"></a><!-- doxytag: member="MIPOMDP::searchNewFrameAtGridPoint" ref="a966a7855d80548dd1d132ca048902817" args="(IplImage *grayFrame, CvPoint searchPoint)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">CvPoint MIPOMDP::searchNewFrameAtGridPoint </td>
          <td>(</td>
          <td class="paramtype">IplImage *&#160;</td>
          <td class="paramname"> <em>grayFrame</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">CvPoint&#160;</td>
          <td class="paramname"> <em>searchPoint</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Search function: Search an image (or frame of video) at a location decided by the calling program. and update belief based on what was found. </p>
<p>Since this is taken to be a new frame, the IPP knows not to use same- frame optimizations.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">grayFrame</td><td>The image to search. Must be of type IPL_DEPTH_8U, 1 channel.</td></tr>
    <tr><td class="paramname">searchPoint</td><td>The grid location to center the digital fovea for further image processing.</td></tr>
  </table>
  </dd>
</dl>
<dl class="return"><dt><b>Returns:</b></dt><dd>The most likely location of the search target. </dd></dl>

</div>
</div>
<a class="anchor" id="ab51fd25d10e5dfda3d67a32f46c7c6a7"></a><!-- doxytag: member="MIPOMDP::setGeneratePreview" ref="ab51fd25d10e5dfda3d67a32f46c7c6a7" args="(int flag)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setGeneratePreview </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>flag</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: Turns on/off the code that modifies foveaRepresentation to visualize the process of fixating. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">flag</td><td>Set to 0 if visualization is not desired (more efficient) or to 1 if visualization is desired. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a0b3c9a261086b2591814522bf1716a01"></a><!-- doxytag: member="MIPOMDP::setHaarCascadeMinSize" ref="a0b3c9a261086b2591814522bf1716a01" args="(int size)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setHaarCascadeMinSize </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>size</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with Object Detector: Sets the minimum patch size at which the classifier searches for the object. By default, this is 0, meaning that the smallest size appropriate to XML file is used. In the case of the frontal face detector provided, this happens to be 20x20 pixels. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">size</td><td>The width/height of the smallest patches to try to detect the object. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a0f6fd2e5c38bcc8d4565d5925f884ed0"></a><!-- doxytag: member="MIPOMDP::setHaarCascadeScaleFactor" ref="a0f6fd2e5c38bcc8d4565d5925f884ed0" args="(double factor)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setHaarCascadeScaleFactor </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>factor</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with Object Detector: Sets the factor by which the image-patch-search-scale is increased. Should be greater than 1. By default, the scale factor is 1.1, meaning that faces are searched for at sizes that increase by 10%. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">factor</td><td>The size-granularity of object search. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ab7119b3aa1176b7119fa2fa7af2b3814"></a><!-- doxytag: member="MIPOMDP::setHeuristicPolicyParameters" ref="ab7119b3aa1176b7119fa2fa7af2b3814" args="(double softmaxGain, double boxSize)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setHeuristicPolicyParameters </td>
          <td>(</td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>softmaxGain</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"> <em>boxSize</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with CLP: Tell the CLP the shape of the convolution kernel to use. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">softmaxGain</td><td>Has the following effect for: </p>
<ul>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9c48f5d9926d94bc565018d32b718df5">ConvolutionalLogisticPolicy::BOX</a> - The kernel is a square with integral=softmaxGain. </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9fcfbe19d01b9a7ba4001b16f4703e4f">ConvolutionalLogisticPolicy::GAUSSIAN</a> - The kernel is a Gaussian with integral=softmaxGain </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9bff1e7f3d490742137ea18ce13b2de0">ConvolutionalLogisticPolicy::IMPULSE</a> - The kernel is an impulse response with value softmaxGain </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#ab71e06b690b9ad2f0e2bc4d133b08d35">ConvolutionalLogisticPolicy::MAX</a> - No effect</li>
</ul>
</td></tr>
    <tr><td class="paramname">boxSize</td><td>Has the following effect for: </p>
<ul>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9c48f5d9926d94bc565018d32b718df5">ConvolutionalLogisticPolicy::BOX</a> - The kernel is a boxSize x boxSize grid-cell square. </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9fcfbe19d01b9a7ba4001b16f4703e4f">ConvolutionalLogisticPolicy::GAUSSIAN</a> - The kernel is a Gaussian with standard-deviation boxSize grid-cells. </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9bff1e7f3d490742137ea18ce13b2de0">ConvolutionalLogisticPolicy::IMPULSE</a> - No effect </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#ab71e06b690b9ad2f0e2bc4d133b08d35">ConvolutionalLogisticPolicy::MAX</a> - No effect </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ac5d779f6f0296fabb64ee64eaa656a03"></a><!-- doxytag: member="MIPOMDP::setMinSize" ref="ac5d779f6f0296fabb64ee64eaa656a03" args="(CvSize minsize)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setMinSize </td>
          <td>(</td>
          <td class="paramtype">CvSize&#160;</td>
          <td class="paramname"> <em>minsize</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: Set the minimum allowed subImageSize. </p>
<p>If the subImageSize is too small (below getMinSize()), the smallest scale is dropped and subImageSize is scaled up proportionally to the next scale. This process is repeated until subImageSIze is greater than getMinSize(). By default, minSize is 60x40.</p>
<p>The will have no effect on the current subImageSize, or getUsedScales() until <a class="el" href="class_m_i_p_o_m_d_p.html#a68a65f637a16ddef96fc9d99b46cd289" title="Interface with IPP: Change the size of the input image and the downsampled image patches. Omitting a newSubImageSize causes the smallest-used-scale to have a 1-1 pixel mapping with the downsampled image patch -- i.e. information is not lost in the smallest scale.">changeInputImageSize()</a> is called.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">minsize</td><td>The minimum allowed subImageSize. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a8431c67294dee47ba23a24f8838cab91"></a><!-- doxytag: member="MIPOMDP::setObjectDetectorSource" ref="a8431c67294dee47ba23a24f8838cab91" args="(std::string newFileName)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setObjectDetectorSource </td>
          <td>(</td>
          <td class="paramtype">std::string&#160;</td>
          <td class="paramname"> <em>newFileName</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Change the file used by the object detector for doing detecting. This is critical if a weights file is located at an absolute path that may have changed from training time. </p>
<p>When an <a class="el" href="class_object_detector.html" title="Auxilliary Tool: A virtual class for providing the skeleton for specific object detectors.">ObjectDetector</a> is loaded from disk, it will try to reload its weights file from the same source used in training. If this fails, a warning will be printed, and the detector's source will need to be set. </p>

</div>
</div>
<a class="anchor" id="af742b9e8b1c5f6b915eb7cedca8f9dfb"></a><!-- doxytag: member="MIPOMDP::setPolicy" ref="af742b9e8b1c5f6b915eb7cedca8f9dfb" args="(int policyNumber)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setPolicy </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>policyNumber</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with CLP: Tell the CLP what kind of convolution policy to use in the future. </p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">policyNumber</td><td>Must be one of: </p>
<ul>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9c48f5d9926d94bc565018d32b718df5">ConvolutionalLogisticPolicy::BOX</a> </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9fcfbe19d01b9a7ba4001b16f4703e4f">ConvolutionalLogisticPolicy::GAUSSIAN</a> </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#a9bff1e7f3d490742137ea18ce13b2de0">ConvolutionalLogisticPolicy::IMPULSE</a> </li>
<li><a class="el" href="class_convolutional_logistic_policy.html#ab71e06b690b9ad2f0e2bc4d133b08d35">ConvolutionalLogisticPolicy::MAX</a> </li>
</ul>
</td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a3e1138e295cbc03764334319661fdfed"></a><!-- doxytag: member="MIPOMDP::setTargetCanMove" ref="a3e1138e295cbc03764334319661fdfed" args="(int flag)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::setTargetCanMove </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>flag</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>: Tell <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> whether there is a possiblity that the target can move. When searching a static frame, set this to 0. When searching sequential frames of a movie, set this to 1. </p>
<p>By default, a simple dynamical model is assumed. The object is expected to move with brownian motion, with a small probability of jumping randomly anywhere in the image.</p>
<p>NOTE: If a frame is searched with one of the search functions that performs multiple fixations, dynamics are automatically, temporarily disabled. At the end of the function call, the targetCanMove is restored to its former state.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">flag</td><td>Set to 1 (default) if searching sequential frames of a movie. Set to 0 if searching a static image. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a11b6d6969860683c89170bfd6a85337f"></a><!-- doxytag: member="MIPOMDP::trainObservationModel" ref="a11b6d6969860683c89170bfd6a85337f" args="(ImageDataSet *trainingSet)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::trainObservationModel </td>
          <td>(</td>
          <td class="paramtype"><a class="el" href="class_image_data_set.html">ImageDataSet</a> *&#160;</td>
          <td class="paramname"> <em>trainingSet</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with Observation Model: Fit a multinomial observation model by recording the object detector output at different fixation points, given known face locations contained in an <a class="el" href="class_image_data_set.html" title="Auxilliary Tool: A data structure for maintaining a list of image files and image labels to facilitat...">ImageDataSet</a>. </p>
<p>Resets the current observation model to a uniform prior before tabulating the data.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">trainingSet</td><td>An iamge dataset, consisting of image files and labeled object locations. The first two indices (0, 1) of all labels in the trainingSet should be the width and height (respectively) of the object's center in the image. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="a798586d237354b3106cfc945d38aceb3"></a><!-- doxytag: member="MIPOMDP::useSameFrameOptimizations" ref="a798586d237354b3106cfc945d38aceb3" args="(int flag)" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void MIPOMDP::useSameFrameOptimizations </td>
          <td>(</td>
          <td class="paramtype">int&#160;</td>
          <td class="paramname"> <em>flag</em>&#160;)</td>
          <td></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with IPP: Set whether same-frame optimizations are being used. </p>
<p>Under certain conditions, the computation needed to search a frame a second time are less than the computations needed to search it a first time. In these conditions, the same-frame optimizations will automatically be used. However, this requires setting setNewImage() each time the image to search changes (i.e. a new frame). If you are in a situation in which you know that each frame will only be fixated at one point, you may wish to turn same-frame optimizations off.</p>
<p>Generally same-frame optimizations should not be turned on unless you know that they were turned on automatically. Turning them on when inappropriate will lead to incorrect behavior. In general, it is appropriate to turn them on if the first scale (largest scale) in the IPP is the same size as entire visual field.</p>
<dl><dt><b>Parameters:</b></dt><dd>
  <table class="params">
    <tr><td class="paramname">flag</td><td>If 0, Same Frame Optimizations will not be used. If 1, Same Frame Optimizations will be used regardless of whether or not it's appropriate. Be careful setting this to 1. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<hr/><h2>Member Data Documentation</h2>
<a class="anchor" id="a240fdf3ebddc3870bd8d00bd6a3344bb"></a><!-- doxytag: member="MIPOMDP::currentBelief" ref="a240fdf3ebddc3870bd8d00bd6a3344bb" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">IplImage* <a class="el" href="class_m_i_p_o_m_d_p.html#a240fdf3ebddc3870bd8d00bd6a3344bb">MIPOMDP::currentBelief</a></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: Access the current belief distribution directly. Changes each time one of the search functions is called. </p>
<p>This is representd as an image of type IPL_DEPTH_32F, 1 channle. This is so that the belief-distribution can be visualized directly in a GUI. </p>

</div>
</div>
<a class="anchor" id="a2a765ad1957d673ffaa0d6e69f0bb410"></a><!-- doxytag: member="MIPOMDP::foveaRepresentation" ref="a2a765ad1957d673ffaa0d6e69f0bb410" args="" -->
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">IplImage* <a class="el" href="class_m_i_p_o_m_d_p.html#a2a765ad1957d673ffaa0d6e69f0bb410">MIPOMDP::foveaRepresentation</a></td>
        </tr>
      </table>
</div>
<div class="memdoc">

<p>Interface with <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> State: A visually informative representation of the IPP Foveal represention. </p>
<p>This is meant as an image that is appropriate for display in a GUI, to visualize the algorithm in action. The image has the same size as inputImageSize. In order to increase efficiency, generation of this visualization should be disabled if it is not going to be accessed. This can be achieved by calling setGeneratePreview(0). </p>

</div>
</div>
<hr/>The documentation for this class was generated from the following files:<ul>
<li>/Users/nick/projects/NickThesis/Code/OpenCV/src/MIPOMDP.h</li>
<li>/Users/nick/projects/NickThesis/Code/OpenCV/src/MIPOMDP.cpp</li>
</ul>
</div>
<hr class="footer"/><address class="footer"><small>Generated on Thu Dec 9 2010 16:05:49 for Nick's Machine Perception Toolbox by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.2 </small></address>
</body>
</html>
