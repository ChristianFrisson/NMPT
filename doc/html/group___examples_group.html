<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Nick&#39;s Machine Perception Toolbox: Example Programs</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<!-- Generated by Doxygen 1.7.2 -->
<div class="navigation" id="top">
  <div class="tabs">
    <ul class="tablist">
      <li><a href="main.html"><span>Main&#160;Page</span></a></li>
      <li><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
    </ul>
  </div>
</div>
<div class="header">
  <div class="headertitle">
<h1>Example Programs</h1>  </div>
</div>
<div class="contents">
<table class="memberdecls">
</table>


<h3><a class="anchor" id="cvprspeed_page"></a>CVPRTestSpeed</h3><p>Reproduce the speed results from Butko and Movellan, CVPR 09 on your own machine.</p>
<p>CVPRTestSpeed</p>
<p>To Run: <br/>
 (1) Uncompress and Expand the included GENKI R2009a dataset. Make sure the GENKI-R2009a folder is in the data directory: <br/>
 <code> &gt;&gt; tar -xzvf data/GENKI-R2009a.tgz -C data/<br/>
 </code> <br/>
 (2) Run the program.<br/>
 <code> &gt;&gt; bin/CVPRTestSpeed.</code></p>
<p><b>Description:</b> </p>
<p>This example program is contained in "CVPRTestSpeed.cpp". Following the proecdure in Butko and Movellan, CVPR 2009, it calculates the speed and accuracy of plain Viola-Jones search, and of MIPOMDP-wrapped Viola-Jones search on the GENKI-SZSL subset of the GENKI data set. The results are computed using 7-Fold cross-validation. The included Multionomial observation models (data/MIPOMDPData-21x21-4Scales-Holdout[0-6].txt) were compted using 3000/3500 of the GENKI-SZSL images. Each file has a different 500 images held out. This program evaluates each image using the model that was created when this image was held out -- i.e. it was not used to fit the model parameters.</p>
<p>After each image is searched, several statistics of performance for the current image are printed, separated by commas:</p>
<ul>
<li><a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> Search Time </li>
<li>VJ Search Time </li>
<li><a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> Distance from Most Likely Face Location to True Face Location </li>
<li>VJ Distance from Most Likely Face Location to True Face Location </li>
<li>Image Width </li>
<li>Image Height </li>
<li>Estimated Probability that Face is really at Face Location </li>
<li>Posterior Belief Distribution Negative Entropy.</li>
</ul>
<p>Then, statistics of average performance are printed.</p>
<p><a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> is an extension of the IPOMDP Infomax Model of Eye-movment in Butko and Movellan, 2008; Najemnik and Geisler, 2005 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). </p>


<h3><a class="anchor" id="cvprtrain_page"></a>CVPRTrainModels</h3><p>Reproduce the Multinomial Observation Models used to generate results in Butko and Movellan, CVPR 09 on your own machine. This file is included for instructional purposes -- the files that it creates are already included in the data directory (data/MIPOMDPData-21x21-4Scales-*.txt).</p>
<p>CVPRTrainModels</p>
<p>To Run: <br/>
 (1) Uncompress and Expand the included GENKI R2009a dataset. Make sure the GENKI-R2009a folder is in the data directory: <br/>
 <code> &gt;&gt; tar -xzvf data/GENKI-R2009a.tgz -C data/<br/>
 </code> <br/>
 (2) Run the program.<br/>
 <code> &gt;&gt; bin/CVPRTrainModels.</code></p>
<p><b>Description:</b> </p>
<p>This example program is contained in "CVPRTrainModels.cpp". Following the proecdure in Butko and Movellan, CVPR 2009, it calculates the coefficients of many multinomial distributions based on object detector performance. As a result of running this program, several MIPOMDPData text files are generated and saved to the data/ directory:</p>
<ul>
<li>data/MIPOMDPData-21x21-4Scales-AllImages.txt - Calculates parameters using all 3500 images in the GENKI-SZSL Directory. </li>
<li>data/MIPOMDPData-21x21-4Scales-ImageSet[0-6].txt - Calculates parameters using non-overlapping blocks of 500 images each. </li>
<li>data/MIPOMDPData-21x21-4Scales-HoldoutSet[0-6].txt - Calculates parameters using 3000 images (all but the 500 included in ImageSet[0-6].txt ; I.e. HoldoutSet0 uses all but the first 500 images, which are used in ImageSet0. HoldoutSet6 uses all but the last 500 images, which are used in ImageSet6. </li>
<li>data/MIPOMDPData-21x21-4Scales-NoTraining.txt - Uses multionomials with parameters set by a heuristic function. These parameters come from the default <a class="el" href="class_multinomial_observation_model.html" title="Auxilliary Tool: Multionomial Observation Model, as described in Butko and Movellan, CVPR 2009 (see Related Publications). This data structure maintains the mapping between object detector output and the probability that the target object is located at every grid-cell.">MultinomialObservationModel</a> constructor.</li>
</ul>
<p>The files created can be used to examine the Multinomial Observation Model Parameters directly, or they can be loaded as <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> Objects that can be used to search for objects. The included program <a class="el" href="group___examples_group.html">CVPRTestSpeed</a> uses the HoldoutSet[0-6] models. All other example programs use the AllImages model.</p>
<p><a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> is an extension of the IPOMDP Infomax Model of Eye-movment in Butko and Movellan, 2008; Najemnik and Geisler, 2005 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). </p>


<h3><a class="anchor" id="fastsun_page"></a>FastSUN</h3><p>An advanced example program illustrating the use of the FastSaliency class. <a class="el" href="group___examples_group.html">SimpleSaliencyExample</a> is considerably simpler, and should be reviewed first.</p>
<p>Fast Saliency Using Natural-Statistics (FastSUN)</p>
<p>To Run Using the Included Movie: <br/>
 <code> &gt;&gt; bin/FastSUN data/HDMovieClip.avi</code></p>
<p>To Run Using an Attached Camera: <br/>
 <code> &gt;&gt; bin/FastSUN </code></p>
<p>To Quit: <br/>
 Press the 'q' key from the video window.</p>
<p>To Pause: <br/>
 Press the 'p' key from the main window.</p>
<p><b>Description:</b> </p>
<p>This example program is contained in "FastSUN.cpp". It is significantly more advanced than <a class="el" href="group___examples_group.html">SimpleSaliencyExample</a>, and demonstrates the effect of the various parameters of the FastSUN algorithm. It can be run on any movie that OpenCV can open, as well as any camera that OpenCV has access to. Calling the program without any arguments causes the program to search for a camera. The first argument to the program is taken as the location of a file to play back. An example movie file, <code>HDMovieClip.avi</code>, is included in the data directory.</p>
<p>The program opens two windows. One displays the input/output of the saliency algorithm as well as the the time required for each iteration. The second displays a set of sliders that change the parameters of the FastSUN algorithm. These are:</p>
<ul>
<li><b>Spatial Scales: </b> The basic spatial features of the FastSUN algorithm are spatial contrast features of increasing size. By increasing the number of spatial scales, you increase the range of the extent of these Difference of Box features. </li>
<li><b>Temporal Scales: </b> The basic temporal features of the FastSUN algorithm look at temporal changes over varying time scales. By increasing the number of temporal scales, you increase the amounts timescales that the algorithm looks for changes over (e.g. slow changes, fast changes). NOTE: As the number of Temporal scales grows, motion starts to dominate the saliency map. </li>
<li><b> Spatial Size: </b> The size of the minimum spatial scale to look for local contrast. </li>
<li><b> Temporal Falloff:</b> Short temporal falloffs are sensetive to quickly varying features. Long temporal falloffs are sensitive to slowly varying features. </li>
<li><b> Image Size: </b> The size of the saliency map. </li>
<li><b> Distribution Power: </b> The exponent of the Generalized-Gaussian feature distribution. Changing this value significantly decreases the speed of the algorithm. </li>
<li><b> Use Spatial: </b> If this is turned off, only the temporal variation of image features is used to calculate the saliency map. </li>
<li><b> Use Temporal: </b> If this is turned off, temporal variation is not considered, and only spatial contrast features are used. </li>
<li><b> Use Color Contrast: </b> If this is turned on, in addition to image intensity contrast, red-green contrast is used for saliency, and also blue-yellow contrast. </li>
<li><b> Estimate Histogram: </b> Build a statistical model of each spatio-temporal feature based on the incoming images. </li>
<li><b> Use Histogram: </b> Use the estimated histogram parameters, rather than the default statistical model.</li>
</ul>
<p>Changing any of the first 5 sliders entails deleting the current saliency tracker and reinitializing a new one (because it changes the structure of the underlying memory represenations). Changing any of the last five sliders can be done an an existing saliency object, and only affects the computations performed.</p>
<p>In Butko et al., ICRA 2008, a simpler version of FastSUN was presented. It can be reproduced in the current parameters by turning off the following settings: Use Spatial, Use Color Contrast, Estimate Histogram, Use Histogram.</p>
<p>FastSUN is an efficient implementation of Zhang et al.'s SUN algorithm (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). </p>


<h3><a class="anchor" id="fastsunimage_page"></a>FastSUNImage</h3><p>An advanced example program illustrating the use of the FastSaliency class. <a class="el" href="group___examples_group.html">SimpleSaliencyExample</a> is considerably simpler, and should be reviewed first.</p>
<p>Fast Saliency Using Natural-Statistics (FastSUN)</p>
<p>To Run Using the Included Image: <br/>
 <code> &gt;&gt; bin/FastSUNImage data/HDImage.jpg</code></p>
<p>To Quit: <br/>
 Press the 'q' key from the video window.</p>
<p>To Pause: <br/>
 Press the 'p' key from the main window.</p>
<p><b>Description:</b> </p>
<p>This example program is contained in "FastSUNImage.cpp". It is significantly more advanced than <a class="el" href="group___examples_group.html">SimpleSaliencyExample</a>, and demonstrates the effect of the various parameters of the FastSUN algorithm. It can be run on any Image that OpenCV can open. An image file must be provided as a command-line argument. An example image file, <code>HDImage.jpg</code>, is included in the data directory.</p>
<p>The program opens two windows. One displays the input/output of the saliency algorithm as well as the the time required for each iteration. The second displays a set of sliders that change the parameters of the FastSUN algorithm. These are:</p>
<ul>
<li><b>Spatial Scales: </b> The basic spatial features of the FastSUN algorithm are spatial contrast features of increasing size. By increasing the number of spatial scales, you increase the range of the extent of these Difference of Box features. </li>
<li><b>Temporal Scales: </b> The basic temporal features of the FastSUN algorithm look at temporal changes over varying time scales. By increasing the number of temporal scales, you increase the amounts timescales that the algorithm looks for changes over (e.g. slow changes, fast changes). NOTE: As the number of Temporal scales grows, motion starts to dominate the saliency map. </li>
<li><b> Spatial Size: </b> The size of the minimum spatial scale to look for local contrast. </li>
<li><b> Temporal Falloff:</b> Short temporal falloffs are sensetive to quickly varying features. Long temporal falloffs are sensitive to slowly varying features. </li>
<li><b> Image Size: </b> The size of the saliency map. </li>
<li><b> Distribution Power: </b> The exponent of the Generalized-Gaussian feature distribution. Changing this value significantly decreases the speed of the algorithm. </li>
<li><b> Use Spatial: </b> If this is turned off, only the temporal variation of image features is used to calculate the saliency map. </li>
<li><b> Use Temporal: </b> If this is turned off, temporal variation is not considered, and only spatial contrast features are used. </li>
<li><b> Use Color Contrast: </b> If this is turned on, in addition to image intensity contrast, red-green contrast is used for saliency, and also blue-yellow contrast. </li>
<li><b> Estimate Histogram: </b> Build a statistical model of each spatio-temporal feature based on the incoming images. </li>
<li><b> Use Histogram: </b> Use the estimated histogram parameters, rather than the default statistical model.</li>
</ul>
<p>Changing any of the first 5 sliders entails deleting the current saliency tracker and reinitializing a new one (because it changes the structure of the underlying memory represenations). Changing any of the last five sliders can be done an an existing saliency object, and only affects the computations performed.</p>
<p>In Butko et al., ICRA 2008, a simpler version of FastSUN was presented. It can be reproduced in the current parameters by turning off the following settings: Use Spatial, Use Color Contrast, Estimate Histogram, Use Histogram.</p>
<p>FastSUN is an efficient implementation of Zhang et al.'s SUN algorithm (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). </p>


<h3><a class="anchor" id="foveatedfacetracker_page"></a>FoveatedFaceTracker</h3><p>A slightly more complex program using the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class: takes a few more input types than the simple face tracker, and displays more information.</p>
<p>First Full-featured Example of Multinomial-Infomax-POMDP for Faster Face Tracking (<a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>), from Butko and Movellan, 2009. (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>).</p>
<p>To Run: <br/>
 <code> &gt;&gt; bin/FoveatedFaceTracker [optional-path-to-movie-file]</code></p>
<p>To Quit: <br/>
 Press the 'q' key from the video window.</p>
<p><b>Description:</b> </p>
<p>This example program is contained in "FoveatedFaceTracker.cpp". It demostrates some of the internal processes of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class. It takes as input an attached camera (if no input arguments are provided), or any movie file that OpenCV can read.</p>
<p>It displays the result of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> tracking algorithm as well as optional visualizations of the algorithm.</p>
<p>In the main program window, the display is controlled by the following keys: </p>
<ul>
<li>'q': Quit. </li>
<li>'t': Toggle display of probabilities and face counts as text. </li>
<li>'b': Toggle display of belief map. </li>
<li>'f': Toggle display of framerate. </li>
<li>'h': Toggle display of hi-res full-frame search (disables belief map). </li>
<li>'r': Reset the belief about the face location.</li>
</ul>
<p>Tip: If the output is not changing, select the display window and try moving the mouse. </p>


<h3><a class="anchor" id="foveatedfacetrackerimage_page"></a>FoveatedFaceTrackerImage</h3><p>A slightly more complex program using the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class: takes any image OpenCV reads as input, and animates visual search.</p>
<p>First Full-featured Example of Multinomial-Infomax-POMDP for Faster Face Tracking (<a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>), from Butko and Movellan, 2009. (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>).</p>
<p>To Run: <br/>
 <code> &gt;&gt; bin/FoveatedFaceTracker [required-path-to-image-file]</code></p>
<p>To Quit: <br/>
 Press the 'q' key from the video window.</p>
<p><b>Description:</b> </p>
<p>This example program is contained in "FoveatedFaceTracker.cpp". It demostrates some of the internal processes of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class. It takes as input an attached camera (if no input arguments are provided), or any movie file that OpenCV can read.</p>
<p>It displays the result of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> tracking algorithm as well as optional visualizations of the algorithm.</p>
<p>In the main program window, the display is controlled by the following keys: </p>
<ul>
<li>'q': Quit. </li>
<li>'t': Toggle display of probabilities and face counts as text. </li>
<li>'b': Toggle display of belief map. </li>
<li>'f': Toggle display of framerate. </li>
<li>'h': Toggle display of hi-res full-frame search (disables belief map). </li>
<li>'r': Reset the belief about the face location.</li>
</ul>
<p>Tip: If the output is not changing, select the display window and try moving the mouse. </p>


<h3><a class="anchor" id="simplefacetracker_page"></a>SimpleFaceTracker</h3><p>An example the simplest program using of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class.</p>
<p>First Simple Example of Multinomial-Infomax-POMDP for Faster Face Tracking (<a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a>), from Butko and Movellan, 2009. (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>).</p>
<p>To Run: <br/>
 <code> &gt;&gt; bin/SimpleFaceTracker</code></p>
<p>To Quit: <br/>
 Press the 'q' key from the video window.</p>
<p><b>Description:</b> </p>
<p>This example program is contained in "SimpleFaceTracker.cpp". It demostrates the simplest usage of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class. It takes as input one of the included example movies. On each frame, it chooses a different region of the video to fixate in a way that is designed to rapidly gather information about the location of the face.</p>
<p>The major steps in creating and using an <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> object are: </p>
<ul>
<li>Load an <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> data file. These data files can be generated using the program "bin/CVPRTrainModels", but examples are already included in the data directory. </li>
<li>Tell the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> data structure what size frame to expect. </li>
<li>Use OpenCV to load an image or a frame of video into memory. </li>
<li>Use OpenCV to convert the image to grayscale. </li>
<li>Call the MIPOMDP's "searchNewFrame" method on the grayscale image. </li>
<li>Access the result via the "foveaRepresentation" member variable.</li>
</ul>
<p>Here are examples of code that this program uses to accomplish these tasks:</p>
<p><b>"Load an MIPOMDP data file."</b></p>
<p><code>MIPOMDP* facetracker = <a class="el" href="class_m_i_p_o_m_d_p.html#ae64349c0e0ce3da39be5c28ff974b8ce" title="Load a saved MIPOMDP from a file. The file should have been saved via the saveToFile() method...">MIPOMDP::loadFromFile</a>("data/MIPOMDPData-21x21-4Scales-AllImages.txt"); </code></p>
<p><b>"Tell the MIPOMDP data structure what size frame to expect."</b></p>
<p><code>facetracker-&gt;changeInputImageSize(cvSize(movieWidth, movieHeight)); </code></p>
<p><b>"Use OpenCV to load an image or a frame of video into memory."</b> First we load the movie into a cvCapture object by</p>
<p><code>CvCapture* movie = cvCreateFileCapture("data/HDMovieClip.avi");</code></p>
<p>Then on every iteration of the main program loop, we query the next frame:</p>
<p><code>current_frame = cvQueryFrame(movie);</code></p>
<p><b>"Use OpenCV to convert the image to grayscale."</b></p>
<p><code>cvCvtColor (current_frame, gray_image, CV_BGR2GRAY);</code></p>
<p><b>"Call the MIPOMDP's "searchNewFrame" method on the grayscale image."</b></p>
<p><code>facetracker-&gt;searchNewFrame(gray_image); </code></p>
<p><b>"Access the result via the 'foveaRepresentation' member variable."</b></p>
<p><code>cvShowImage (WINDOW_NAME, facetracker-&gt;foveaRepresentation); </code></p>
<p>This concludes the tutorial of the simplest usage of the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> class. </p>


<h3><a class="anchor" id="simplesaliency_page"></a>SimpleSaliencyExample</h3><p>An example the simplest program using of the FastSaliency class.</p>
<p>First Simple Example of Fast Saliency Using Natural-Statistics (FastSUN), from Butko et al., 2008. FastSUN is an efficient implementation of Zhang et al.'s SUN algorithm (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>).</p>
<p>To Run: <br/>
 <code> &gt;&gt; bin/SimpleSaliencyExample</code></p>
<p>To Quit: <br/>
 Press the 'q' key from the video window.</p>
<p><b>Description:</b> </p>
<p>This example program is contained in "SimpleSaliencyExample.cpp". It demostrates the simplest usage of the FastSaliency class. It takes as input one of the included example movies. It produces a frame-by-frame saliency map as output.</p>
<p>The major steps in creating and using a FastSaliency object are: </p>
<ul>
<li>Call the constructor with the desired size of the saliency map (in pixels), leaving other parameters as defaults. </li>
<li>Use OpenCV to load an image or a frame of video into memory. </li>
<li>Use OpenCV to convert the image to floating-point, and to resize it to the size of your saliency map. </li>
<li>Call the saliency tracker's "updateSaliency" method on the resized floating-point image. </li>
<li>Access the result via either the "salImageDouble" member variable, or the "salImageFloat" member variable.</li>
</ul>
<p>Here are examples of code that this program uses to accomplish these tasks:</p>
<p><b>"Call the constructor with the desired size of the saliency map (in pixels), leaving other parameters
 as defaults."</b></p>
<p><code>FastSaliency* saltracker = new FastSaliency(saliencyMapWidth, saliencyMapHeight);</code></p>
<p><b>"Use OpenCV to load an image or a frame of video into memory."</b> First we load the movie into a cvCapture object by</p>
<p><code>CvCapture* movie = cvCreateFileCapture("data/HDMovieClip.avi");</code></p>
<p>Then on every iteration of the main program loop, we query the next frame:</p>
<p><code>current_frame = cvQueryFrame(movie);</code></p>
<p><b>"Use OpenCV to convert the image to floating-point image, and to resize it to the size of your saliency map."</b> The program then downsizes the incoming frame to the size of the saliency map, and converts it from the integer format supplied by the avi file to the floating point format expected by the FastSaliency algorithm.</p>
<p><code> cvResize(current_frame, small_color_image, CV_INTER_LINEAR); <br/>
 cvConvert(small_color_image, small_float_image); </code></p>
<p><b>"Call the saliency tracker's "updateSaliency" method on the resized, grayscale image. "</b> Now that the image format is the right size and data type, we invoke the main method of the SaliencyTracker object, "updateSaliency":</p>
<p><code> saltracker-&gt;updateSaliency(small_float_image) ; </code></p>
<p><b>"Access the result via either the 'salImageDouble' member variable, or the 'salImageFloat' member variable."</b> Finally, the results of the saliency tracker can be visualized in the main window</p>
<p><code>cvShowImage (WINDOW_NAME, saltracker-&gt;salImageFloat); </code></p>
<p>This concludes the tutorial of the simplest usage of the FastSaliency class. </p>


<h3><a class="anchor" id="narrowfov_page"></a>TrainNarrowFOVModel</h3><p>Create a restricted field of view model to illustrate how the <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> can simulate an active camera. This file is included for instructional purposes -- the files that it creates are already included in the data directory (data/MIPOMDPData-21x21-3Scales-AllImages.txt).</p>
<p>TrainNarrowFOVModel</p>
<p>To Run: <br/>
 (1) Uncompress and Expand the included GENKI R2009a dataset. Make sure the GENKI-R2009a folder is in the data directory: <br/>
 <code> &gt;&gt; tar -xzvf data/GENKI-R2009a.tgz -C data/<br/>
 </code> <br/>
 (2) Run the program.<br/>
 <code> &gt;&gt; bin/TrainNarrowFOVModel.</code></p>
<p><b>Description:</b> </p>
<p>This example program is contained in "TrainNarrowFOVModel.cpp". Following the proecdure in Butko and Movellan, CVPR 2009, it calculates the coefficients of many multinomial distributions based on object detector performance. As a result of running this program, an MIPOMDPData text file is generated and saved to the data/ directory:</p>
<ul>
<li>data/MIPOMDPData-21x21-4Scales-NarrowFOV.txt - Calculates parameters using all 3500 images in the GENKI-SZSL Directory.</li>
</ul>
<p>The file created can be used to examine the Multinomial Observation Model Parameters directly, or it can be loaded as an <a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> Objects that can be used to search for objects. The included program <a class="el" href="group___examples_group.html">FoveatedFaceTracker</a> uses this model. All other example programs use the AllImages model.</p>
<p><a class="el" href="class_m_i_p_o_m_d_p.html" title="  Machine Perception Primitive:   An implementation of the &quot;Multinomial IPOMDP&quot; algorithm f...">MIPOMDP</a> is an extension of the IPOMDP Infomax Model of Eye-movment in Butko and Movellan, 2008; Najemnik and Geisler, 2005 (see <a class="el" href="bib_page.html#bib_sec">Related Publications</a>). </p>
</div>
<hr class="footer"/><address class="footer"><small>Generated on Thu Dec 9 2010 16:05:47 for Nick's Machine Perception Toolbox by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.2 </small></address>
</body>
</html>
